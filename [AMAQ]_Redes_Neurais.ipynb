{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[AMAQ]_Redes_Neurais - Cópia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDvePSG2u_Bm",
        "colab_type": "text"
      },
      "source": [
        "![IFCE Logo](https://ifce.edu.br/fortaleza/comunicacao-2/logo-ifce-campus-de-fortaleza-horizontal.jpg/@@images/ba68e46f-55df-436f-9bfb-5eeed5d6fa0b.jpeg)\n",
        "\n",
        "**Disciplina:** Aprendizagem de Máquina\n",
        "\n",
        "**Professor:** Amauri Holanda\n",
        "\n",
        "**Aluno:** Keven Carneiro\n",
        "\n",
        "# Redes Neurais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gIg4-vEvtiX",
        "colab_type": "text"
      },
      "source": [
        "**Questão 1.** O arquivo https://www.dropbox.com/s/s2cyx82uxsv03rq/dados-ex5.txt?dl=0 possui amostras de treinamento para um problema de classificação binária com X = $R^2$ e Y = {0, 1}.\n",
        "Neste exercício, você deve avaliar a aplicação de redes neurais do tipo MLP (Multilayer Perceptrons) ao problema proposto. Para isso, utilize validação hold-out para seleção de modelo (mostre os erros de validação para cada configuração avaliada) e plote as superfícies de decisão para a melhor e a pior (segundo o erro de validação) das configurações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ5za3HSdcXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import sklearn\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aP18b1ydZuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://www.dropbox.com/s/s2cyx82uxsv03rq/dados-ex5.txt?dl=1', names=['x1', 'x2', 'y'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afgussa6gOMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(df.drop('y', axis='columns').values).float()\n",
        "y = torch.tensor(df['y']).float()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
        "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3ZKDSkcdomD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Feedforward(torch.nn.Module):\n",
        "  def __init__(self, input_size, output_size, number_layers, number_units):\n",
        "    super(Feedforward, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.number_layers = number_layers\n",
        "    self.number_units = number_units\n",
        "    fc = torch.nn.ModuleList() # fully connected\n",
        "    for i in range(number_layers):\n",
        "      if i == 0:\n",
        "        fc.append(torch.nn.Linear(input_size, number_units))\n",
        "      elif i == number_layers-1:\n",
        "        fc.append(torch.nn.Linear(number_units, output_size))\n",
        "      else:\n",
        "        fc.append(torch.nn.Linear(number_units, number_units))\n",
        "    self.fc = fc\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i in range(self.number_layers):\n",
        "      if i == self.number_layers-1:\n",
        "        x = F.sigmoid(self.fc[i](x))\n",
        "      else:\n",
        "        x = F.relu(self.fc[i](x))\n",
        "    return x"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5zFbdmihsYw",
        "colab_type": "code",
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61a0767d-c359-4ce9-f179-49a1213c363b"
      },
      "source": [
        "number_units = range(2, 101, 15)\n",
        "number_layers = range(2, 10, 1)\n",
        "learning_rates = [10**-x for x in range(5, -3, -1)]\n",
        "epoch = 50\n",
        "\n",
        "best_model = {'model': None, 'optimizer': None, 'loss': sys.maxsize, 'accuracy': 0}\n",
        "worst_model = {'model': None, 'optimizer': None, 'loss': 0, 'accuracy': 0}\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  for number_layer in number_layers:\n",
        "    for number_unit in number_units:\n",
        "      model = Feedforward(2, 1, number_layer, number_unit)\n",
        "      criterion = torch.nn.BCELoss()\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "      model.train()\n",
        "      for epoch in range(1, epoch+1):\n",
        "          optimizer.zero_grad()\n",
        "          # Forward pass\n",
        "          y_pred = model(x_train)\n",
        "          # Compute Loss\n",
        "          loss = criterion(y_pred.squeeze(), y_train)\n",
        "          # Backward pass\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      print(f'#Layers: {number_layer} #Units: {number_unit} LR: {learning_rate} Epoch {epoch}: train loss: {loss.item()}')\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        y_pred = model(x_cv)\n",
        "        accuracy = ((y_pred.squeeze() >= 0.5) == y_cv).sum().item() / y_cv.shape[0]\n",
        "        print(f'#Layers: {number_layer} #Units: {number_unit} LR: {learning_rate} Training loss: {loss.item()} Cross validation accuracy: {accuracy}')\n",
        "\n",
        "        if loss.item() < best_model['loss']:\n",
        "          best_model['model'] = model\n",
        "          best_model['optimizer'] = optimizer\n",
        "          best_model['loss'] = loss.item()\n",
        "          best_model['accuracy'] = accuracy\n",
        "\n",
        "        if loss.item() > worst_model['loss']:\n",
        "          worst_model['model'] = model\n",
        "          worst_model['optimizer'] = optimizer\n",
        "          worst_model['loss'] = loss.item()\n",
        "          worst_model['accuracy'] = accuracy"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning:\n",
            "\n",
            "nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#Layers: 2 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.7381122708320618\n",
            "#Layers: 2 #Units: 2 LR: 1e-05 Training loss: 0.7381122708320618 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.7710756063461304\n",
            "#Layers: 2 #Units: 17 LR: 1e-05 Training loss: 0.7710756063461304 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.7807587385177612\n",
            "#Layers: 2 #Units: 32 LR: 1e-05 Training loss: 0.7807587385177612 Cross validation accuracy: 0.3431952662721893\n",
            "#Layers: 2 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.6551284193992615\n",
            "#Layers: 2 #Units: 47 LR: 1e-05 Training loss: 0.6551284193992615 Cross validation accuracy: 0.7100591715976331\n",
            "#Layers: 2 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6901153922080994\n",
            "#Layers: 2 #Units: 62 LR: 1e-05 Training loss: 0.6901153922080994 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.7057189345359802\n",
            "#Layers: 2 #Units: 77 LR: 1e-05 Training loss: 0.7057189345359802 Cross validation accuracy: 0.20710059171597633\n",
            "#Layers: 2 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.7687099575996399\n",
            "#Layers: 2 #Units: 92 LR: 1e-05 Training loss: 0.7687099575996399 Cross validation accuracy: 0.38461538461538464\n",
            "#Layers: 3 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.6971358060836792\n",
            "#Layers: 3 #Units: 2 LR: 1e-05 Training loss: 0.6971358060836792 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.7066282629966736\n",
            "#Layers: 3 #Units: 17 LR: 1e-05 Training loss: 0.7066282629966736 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.6983357667922974\n",
            "#Layers: 3 #Units: 32 LR: 1e-05 Training loss: 0.6983357667922974 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.6978729963302612\n",
            "#Layers: 3 #Units: 47 LR: 1e-05 Training loss: 0.6978729963302612 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6909076571464539\n",
            "#Layers: 3 #Units: 62 LR: 1e-05 Training loss: 0.6909076571464539 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.7050958871841431\n",
            "#Layers: 3 #Units: 77 LR: 1e-05 Training loss: 0.7050958871841431 Cross validation accuracy: 0.20118343195266272\n",
            "#Layers: 3 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.7126348614692688\n",
            "#Layers: 3 #Units: 92 LR: 1e-05 Training loss: 0.7126348614692688 Cross validation accuracy: 0.1893491124260355\n",
            "#Layers: 4 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.6997800469398499\n",
            "#Layers: 4 #Units: 2 LR: 1e-05 Training loss: 0.6997800469398499 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.6931865215301514\n",
            "#Layers: 4 #Units: 17 LR: 1e-05 Training loss: 0.6931865215301514 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.6902818083763123\n",
            "#Layers: 4 #Units: 32 LR: 1e-05 Training loss: 0.6902818083763123 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.7065727710723877\n",
            "#Layers: 4 #Units: 47 LR: 1e-05 Training loss: 0.7065727710723877 Cross validation accuracy: 0.514792899408284\n",
            "#Layers: 4 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.7013451457023621\n",
            "#Layers: 4 #Units: 62 LR: 1e-05 Training loss: 0.7013451457023621 Cross validation accuracy: 0.14792899408284024\n",
            "#Layers: 4 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.6931504011154175\n",
            "#Layers: 4 #Units: 77 LR: 1e-05 Training loss: 0.6931504011154175 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.6902220249176025\n",
            "#Layers: 4 #Units: 92 LR: 1e-05 Training loss: 0.6902220249176025 Cross validation accuracy: 0.5621301775147929\n",
            "#Layers: 5 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.7040720582008362\n",
            "#Layers: 5 #Units: 2 LR: 1e-05 Training loss: 0.7040720582008362 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.6963808536529541\n",
            "#Layers: 5 #Units: 17 LR: 1e-05 Training loss: 0.6963808536529541 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.6944612264633179\n",
            "#Layers: 5 #Units: 32 LR: 1e-05 Training loss: 0.6944612264633179 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.697722852230072\n",
            "#Layers: 5 #Units: 47 LR: 1e-05 Training loss: 0.697722852230072 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6914084553718567\n",
            "#Layers: 5 #Units: 62 LR: 1e-05 Training loss: 0.6914084553718567 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.6951271295547485\n",
            "#Layers: 5 #Units: 77 LR: 1e-05 Training loss: 0.6951271295547485 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.6924723386764526\n",
            "#Layers: 5 #Units: 92 LR: 1e-05 Training loss: 0.6924723386764526 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.6982743740081787\n",
            "#Layers: 6 #Units: 2 LR: 1e-05 Training loss: 0.6982743740081787 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.6936967968940735\n",
            "#Layers: 6 #Units: 17 LR: 1e-05 Training loss: 0.6936967968940735 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.6948026418685913\n",
            "#Layers: 6 #Units: 32 LR: 1e-05 Training loss: 0.6948026418685913 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.6934735178947449\n",
            "#Layers: 6 #Units: 47 LR: 1e-05 Training loss: 0.6934735178947449 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6951681971549988\n",
            "#Layers: 6 #Units: 62 LR: 1e-05 Training loss: 0.6951681971549988 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.6939224004745483\n",
            "#Layers: 6 #Units: 77 LR: 1e-05 Training loss: 0.6939224004745483 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.6936629414558411\n",
            "#Layers: 6 #Units: 92 LR: 1e-05 Training loss: 0.6936629414558411 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.6930533647537231\n",
            "#Layers: 7 #Units: 2 LR: 1e-05 Training loss: 0.6930533647537231 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.6958613991737366\n",
            "#Layers: 7 #Units: 17 LR: 1e-05 Training loss: 0.6958613991737366 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.6941143274307251\n",
            "#Layers: 7 #Units: 32 LR: 1e-05 Training loss: 0.6941143274307251 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.6940887570381165\n",
            "#Layers: 7 #Units: 47 LR: 1e-05 Training loss: 0.6940887570381165 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6930184960365295\n",
            "#Layers: 7 #Units: 62 LR: 1e-05 Training loss: 0.6930184960365295 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.6928829550743103\n",
            "#Layers: 7 #Units: 77 LR: 1e-05 Training loss: 0.6928829550743103 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.6946972012519836\n",
            "#Layers: 7 #Units: 92 LR: 1e-05 Training loss: 0.6946972012519836 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.6930299401283264\n",
            "#Layers: 8 #Units: 2 LR: 1e-05 Training loss: 0.6930299401283264 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.6943688988685608\n",
            "#Layers: 8 #Units: 17 LR: 1e-05 Training loss: 0.6943688988685608 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.6936229467391968\n",
            "#Layers: 8 #Units: 32 LR: 1e-05 Training loss: 0.6936229467391968 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.6937459111213684\n",
            "#Layers: 8 #Units: 47 LR: 1e-05 Training loss: 0.6937459111213684 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6926068663597107\n",
            "#Layers: 8 #Units: 62 LR: 1e-05 Training loss: 0.6926068663597107 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.6956807374954224\n",
            "#Layers: 8 #Units: 77 LR: 1e-05 Training loss: 0.6956807374954224 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.6939257979393005\n",
            "#Layers: 8 #Units: 92 LR: 1e-05 Training loss: 0.6939257979393005 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 2 LR: 1e-05 Epoch 50: train loss: 0.7026368975639343\n",
            "#Layers: 9 #Units: 2 LR: 1e-05 Training loss: 0.7026368975639343 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 17 LR: 1e-05 Epoch 50: train loss: 0.6931526064872742\n",
            "#Layers: 9 #Units: 17 LR: 1e-05 Training loss: 0.6931526064872742 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 32 LR: 1e-05 Epoch 50: train loss: 0.695260226726532\n",
            "#Layers: 9 #Units: 32 LR: 1e-05 Training loss: 0.695260226726532 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 47 LR: 1e-05 Epoch 50: train loss: 0.6932114362716675\n",
            "#Layers: 9 #Units: 47 LR: 1e-05 Training loss: 0.6932114362716675 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 62 LR: 1e-05 Epoch 50: train loss: 0.6932641863822937\n",
            "#Layers: 9 #Units: 62 LR: 1e-05 Training loss: 0.6932641863822937 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 77 LR: 1e-05 Epoch 50: train loss: 0.6931145191192627\n",
            "#Layers: 9 #Units: 77 LR: 1e-05 Training loss: 0.6931145191192627 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 92 LR: 1e-05 Epoch 50: train loss: 0.6943389773368835\n",
            "#Layers: 9 #Units: 92 LR: 1e-05 Training loss: 0.6943389773368835 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.7153083086013794\n",
            "#Layers: 2 #Units: 2 LR: 0.0001 Training loss: 0.7153083086013794 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6495636105537415\n",
            "#Layers: 2 #Units: 17 LR: 0.0001 Training loss: 0.6495636105537415 Cross validation accuracy: 0.6272189349112426\n",
            "#Layers: 2 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.7479187250137329\n",
            "#Layers: 2 #Units: 32 LR: 0.0001 Training loss: 0.7479187250137329 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.7152315974235535\n",
            "#Layers: 2 #Units: 47 LR: 0.0001 Training loss: 0.7152315974235535 Cross validation accuracy: 0.42011834319526625\n",
            "#Layers: 2 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6918554306030273\n",
            "#Layers: 2 #Units: 62 LR: 0.0001 Training loss: 0.6918554306030273 Cross validation accuracy: 0.4437869822485207\n",
            "#Layers: 2 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.6402388215065002\n",
            "#Layers: 2 #Units: 77 LR: 0.0001 Training loss: 0.6402388215065002 Cross validation accuracy: 0.8284023668639053\n",
            "#Layers: 2 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.7458464503288269\n",
            "#Layers: 2 #Units: 92 LR: 0.0001 Training loss: 0.7458464503288269 Cross validation accuracy: 0.1242603550295858\n",
            "#Layers: 3 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.6924355626106262\n",
            "#Layers: 3 #Units: 2 LR: 0.0001 Training loss: 0.6924355626106262 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6828420162200928\n",
            "#Layers: 3 #Units: 17 LR: 0.0001 Training loss: 0.6828420162200928 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.681712806224823\n",
            "#Layers: 3 #Units: 32 LR: 0.0001 Training loss: 0.681712806224823 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6876199245452881\n",
            "#Layers: 3 #Units: 47 LR: 0.0001 Training loss: 0.6876199245452881 Cross validation accuracy: 0.5680473372781065\n",
            "#Layers: 3 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6714401841163635\n",
            "#Layers: 3 #Units: 62 LR: 0.0001 Training loss: 0.6714401841163635 Cross validation accuracy: 0.4911242603550296\n",
            "#Layers: 3 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.7136927247047424\n",
            "#Layers: 3 #Units: 77 LR: 0.0001 Training loss: 0.7136927247047424 Cross validation accuracy: 0.3254437869822485\n",
            "#Layers: 3 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6860178709030151\n",
            "#Layers: 3 #Units: 92 LR: 0.0001 Training loss: 0.6860178709030151 Cross validation accuracy: 0.6449704142011834\n",
            "#Layers: 4 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.7193028926849365\n",
            "#Layers: 4 #Units: 2 LR: 0.0001 Training loss: 0.7193028926849365 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.7044658660888672\n",
            "#Layers: 4 #Units: 17 LR: 0.0001 Training loss: 0.7044658660888672 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.6932812333106995\n",
            "#Layers: 4 #Units: 32 LR: 0.0001 Training loss: 0.6932812333106995 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6943437457084656\n",
            "#Layers: 4 #Units: 47 LR: 0.0001 Training loss: 0.6943437457084656 Cross validation accuracy: 0.591715976331361\n",
            "#Layers: 4 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6892312169075012\n",
            "#Layers: 4 #Units: 62 LR: 0.0001 Training loss: 0.6892312169075012 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.6835227608680725\n",
            "#Layers: 4 #Units: 77 LR: 0.0001 Training loss: 0.6835227608680725 Cross validation accuracy: 0.8579881656804734\n",
            "#Layers: 4 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6885049939155579\n",
            "#Layers: 4 #Units: 92 LR: 0.0001 Training loss: 0.6885049939155579 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.6901306509971619\n",
            "#Layers: 5 #Units: 2 LR: 0.0001 Training loss: 0.6901306509971619 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6965833902359009\n",
            "#Layers: 5 #Units: 17 LR: 0.0001 Training loss: 0.6965833902359009 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.6991672515869141\n",
            "#Layers: 5 #Units: 32 LR: 0.0001 Training loss: 0.6991672515869141 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6932410001754761\n",
            "#Layers: 5 #Units: 47 LR: 0.0001 Training loss: 0.6932410001754761 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6932792067527771\n",
            "#Layers: 5 #Units: 62 LR: 0.0001 Training loss: 0.6932792067527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.6952421069145203\n",
            "#Layers: 5 #Units: 77 LR: 0.0001 Training loss: 0.6952421069145203 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6949607729911804\n",
            "#Layers: 5 #Units: 92 LR: 0.0001 Training loss: 0.6949607729911804 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.7002589702606201\n",
            "#Layers: 6 #Units: 2 LR: 0.0001 Training loss: 0.7002589702606201 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6978780031204224\n",
            "#Layers: 6 #Units: 17 LR: 0.0001 Training loss: 0.6978780031204224 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.6942490935325623\n",
            "#Layers: 6 #Units: 32 LR: 0.0001 Training loss: 0.6942490935325623 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6934767365455627\n",
            "#Layers: 6 #Units: 47 LR: 0.0001 Training loss: 0.6934767365455627 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6919548511505127\n",
            "#Layers: 6 #Units: 62 LR: 0.0001 Training loss: 0.6919548511505127 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.6933649778366089\n",
            "#Layers: 6 #Units: 77 LR: 0.0001 Training loss: 0.6933649778366089 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6947377920150757\n",
            "#Layers: 6 #Units: 92 LR: 0.0001 Training loss: 0.6947377920150757 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.740075945854187\n",
            "#Layers: 7 #Units: 2 LR: 0.0001 Training loss: 0.740075945854187 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6945538520812988\n",
            "#Layers: 7 #Units: 17 LR: 0.0001 Training loss: 0.6945538520812988 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.699985682964325\n",
            "#Layers: 7 #Units: 32 LR: 0.0001 Training loss: 0.699985682964325 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6941177248954773\n",
            "#Layers: 7 #Units: 47 LR: 0.0001 Training loss: 0.6941177248954773 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6932437419891357\n",
            "#Layers: 7 #Units: 62 LR: 0.0001 Training loss: 0.6932437419891357 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.692719578742981\n",
            "#Layers: 7 #Units: 77 LR: 0.0001 Training loss: 0.692719578742981 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6930242776870728\n",
            "#Layers: 7 #Units: 92 LR: 0.0001 Training loss: 0.6930242776870728 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.7055885791778564\n",
            "#Layers: 8 #Units: 2 LR: 0.0001 Training loss: 0.7055885791778564 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6948203444480896\n",
            "#Layers: 8 #Units: 17 LR: 0.0001 Training loss: 0.6948203444480896 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.6930651068687439\n",
            "#Layers: 8 #Units: 32 LR: 0.0001 Training loss: 0.6930651068687439 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6941900253295898\n",
            "#Layers: 8 #Units: 47 LR: 0.0001 Training loss: 0.6941900253295898 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6929656267166138\n",
            "#Layers: 8 #Units: 62 LR: 0.0001 Training loss: 0.6929656267166138 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.6955322027206421\n",
            "#Layers: 8 #Units: 77 LR: 0.0001 Training loss: 0.6955322027206421 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6938626766204834\n",
            "#Layers: 8 #Units: 92 LR: 0.0001 Training loss: 0.6938626766204834 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 2 LR: 0.0001 Epoch 50: train loss: 0.6933516263961792\n",
            "#Layers: 9 #Units: 2 LR: 0.0001 Training loss: 0.6933516263961792 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 17 LR: 0.0001 Epoch 50: train loss: 0.6951978206634521\n",
            "#Layers: 9 #Units: 17 LR: 0.0001 Training loss: 0.6951978206634521 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 32 LR: 0.0001 Epoch 50: train loss: 0.6933922171592712\n",
            "#Layers: 9 #Units: 32 LR: 0.0001 Training loss: 0.6933922171592712 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 47 LR: 0.0001 Epoch 50: train loss: 0.6958991289138794\n",
            "#Layers: 9 #Units: 47 LR: 0.0001 Training loss: 0.6958991289138794 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 62 LR: 0.0001 Epoch 50: train loss: 0.6930482387542725\n",
            "#Layers: 9 #Units: 62 LR: 0.0001 Training loss: 0.6930482387542725 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 77 LR: 0.0001 Epoch 50: train loss: 0.6930086612701416\n",
            "#Layers: 9 #Units: 77 LR: 0.0001 Training loss: 0.6930086612701416 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 92 LR: 0.0001 Epoch 50: train loss: 0.6946372985839844\n",
            "#Layers: 9 #Units: 92 LR: 0.0001 Training loss: 0.6946372985839844 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.6307151913642883\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Training loss: 0.6307151913642883 Cross validation accuracy: 0.6982248520710059\n",
            "#Layers: 2 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.647396981716156\n",
            "#Layers: 2 #Units: 17 LR: 0.001 Training loss: 0.647396981716156 Cross validation accuracy: 0.8816568047337278\n",
            "#Layers: 2 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6917206048965454\n",
            "#Layers: 2 #Units: 32 LR: 0.001 Training loss: 0.6917206048965454 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6533427238464355\n",
            "#Layers: 2 #Units: 47 LR: 0.001 Training loss: 0.6533427238464355 Cross validation accuracy: 0.7218934911242604\n",
            "#Layers: 2 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.62789386510849\n",
            "#Layers: 2 #Units: 62 LR: 0.001 Training loss: 0.62789386510849 Cross validation accuracy: 0.5384615384615384\n",
            "#Layers: 2 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6925858855247498\n",
            "#Layers: 2 #Units: 77 LR: 0.001 Training loss: 0.6925858855247498 Cross validation accuracy: 0.5680473372781065\n",
            "#Layers: 2 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.7258353233337402\n",
            "#Layers: 2 #Units: 92 LR: 0.001 Training loss: 0.7258353233337402 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.7158869504928589\n",
            "#Layers: 3 #Units: 2 LR: 0.001 Training loss: 0.7158869504928589 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6978853940963745\n",
            "#Layers: 3 #Units: 17 LR: 0.001 Training loss: 0.6978853940963745 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6996819376945496\n",
            "#Layers: 3 #Units: 32 LR: 0.001 Training loss: 0.6996819376945496 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6696472764015198\n",
            "#Layers: 3 #Units: 47 LR: 0.001 Training loss: 0.6696472764015198 Cross validation accuracy: 0.7692307692307693\n",
            "#Layers: 3 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.6727903485298157\n",
            "#Layers: 3 #Units: 62 LR: 0.001 Training loss: 0.6727903485298157 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6908589005470276\n",
            "#Layers: 3 #Units: 77 LR: 0.001 Training loss: 0.6908589005470276 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6808393001556396\n",
            "#Layers: 3 #Units: 92 LR: 0.001 Training loss: 0.6808393001556396 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.7211479544639587\n",
            "#Layers: 4 #Units: 2 LR: 0.001 Training loss: 0.7211479544639587 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6996733546257019\n",
            "#Layers: 4 #Units: 17 LR: 0.001 Training loss: 0.6996733546257019 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6901169419288635\n",
            "#Layers: 4 #Units: 32 LR: 0.001 Training loss: 0.6901169419288635 Cross validation accuracy: 0.5680473372781065\n",
            "#Layers: 4 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6920468211174011\n",
            "#Layers: 4 #Units: 47 LR: 0.001 Training loss: 0.6920468211174011 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.69488525390625\n",
            "#Layers: 4 #Units: 62 LR: 0.001 Training loss: 0.69488525390625 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6932610273361206\n",
            "#Layers: 4 #Units: 77 LR: 0.001 Training loss: 0.6932610273361206 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6967409253120422\n",
            "#Layers: 4 #Units: 92 LR: 0.001 Training loss: 0.6967409253120422 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.7210042476654053\n",
            "#Layers: 5 #Units: 2 LR: 0.001 Training loss: 0.7210042476654053 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6988595724105835\n",
            "#Layers: 5 #Units: 17 LR: 0.001 Training loss: 0.6988595724105835 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6907579898834229\n",
            "#Layers: 5 #Units: 32 LR: 0.001 Training loss: 0.6907579898834229 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6924182176589966\n",
            "#Layers: 5 #Units: 47 LR: 0.001 Training loss: 0.6924182176589966 Cross validation accuracy: 0.6982248520710059\n",
            "#Layers: 5 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.6986570358276367\n",
            "#Layers: 5 #Units: 62 LR: 0.001 Training loss: 0.6986570358276367 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6930596232414246\n",
            "#Layers: 5 #Units: 77 LR: 0.001 Training loss: 0.6930596232414246 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6931769847869873\n",
            "#Layers: 5 #Units: 92 LR: 0.001 Training loss: 0.6931769847869873 Cross validation accuracy: 0.5976331360946746\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.7260510325431824\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Training loss: 0.7260510325431824 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6945657730102539\n",
            "#Layers: 6 #Units: 17 LR: 0.001 Training loss: 0.6945657730102539 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6913740038871765\n",
            "#Layers: 6 #Units: 32 LR: 0.001 Training loss: 0.6913740038871765 Cross validation accuracy: 0.7159763313609467\n",
            "#Layers: 6 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.694244921207428\n",
            "#Layers: 6 #Units: 47 LR: 0.001 Training loss: 0.694244921207428 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.6935845613479614\n",
            "#Layers: 6 #Units: 62 LR: 0.001 Training loss: 0.6935845613479614 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6939137578010559\n",
            "#Layers: 6 #Units: 77 LR: 0.001 Training loss: 0.6939137578010559 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6942169070243835\n",
            "#Layers: 6 #Units: 92 LR: 0.001 Training loss: 0.6942169070243835 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.7398661375045776\n",
            "#Layers: 7 #Units: 2 LR: 0.001 Training loss: 0.7398661375045776 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6983580589294434\n",
            "#Layers: 7 #Units: 17 LR: 0.001 Training loss: 0.6983580589294434 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6925641298294067\n",
            "#Layers: 7 #Units: 32 LR: 0.001 Training loss: 0.6925641298294067 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6930989623069763\n",
            "#Layers: 7 #Units: 47 LR: 0.001 Training loss: 0.6930989623069763 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.6937108039855957\n",
            "#Layers: 7 #Units: 62 LR: 0.001 Training loss: 0.6937108039855957 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6941192746162415\n",
            "#Layers: 7 #Units: 77 LR: 0.001 Training loss: 0.6941192746162415 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6946811079978943\n",
            "#Layers: 7 #Units: 92 LR: 0.001 Training loss: 0.6946811079978943 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.6941843032836914\n",
            "#Layers: 8 #Units: 2 LR: 0.001 Training loss: 0.6941843032836914 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6959543228149414\n",
            "#Layers: 8 #Units: 17 LR: 0.001 Training loss: 0.6959543228149414 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6939598321914673\n",
            "#Layers: 8 #Units: 32 LR: 0.001 Training loss: 0.6939598321914673 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6930591464042664\n",
            "#Layers: 8 #Units: 47 LR: 0.001 Training loss: 0.6930591464042664 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.6933462023735046\n",
            "#Layers: 8 #Units: 62 LR: 0.001 Training loss: 0.6933462023735046 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6928360462188721\n",
            "#Layers: 8 #Units: 77 LR: 0.001 Training loss: 0.6928360462188721 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6929765343666077\n",
            "#Layers: 8 #Units: 92 LR: 0.001 Training loss: 0.6929765343666077 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 2 LR: 0.001 Epoch 50: train loss: 0.7052212953567505\n",
            "#Layers: 9 #Units: 2 LR: 0.001 Training loss: 0.7052212953567505 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 17 LR: 0.001 Epoch 50: train loss: 0.6930514574050903\n",
            "#Layers: 9 #Units: 17 LR: 0.001 Training loss: 0.6930514574050903 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 32 LR: 0.001 Epoch 50: train loss: 0.6935601234436035\n",
            "#Layers: 9 #Units: 32 LR: 0.001 Training loss: 0.6935601234436035 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 47 LR: 0.001 Epoch 50: train loss: 0.6934317350387573\n",
            "#Layers: 9 #Units: 47 LR: 0.001 Training loss: 0.6934317350387573 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 62 LR: 0.001 Epoch 50: train loss: 0.6934500932693481\n",
            "#Layers: 9 #Units: 62 LR: 0.001 Training loss: 0.6934500932693481 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 77 LR: 0.001 Epoch 50: train loss: 0.6933416724205017\n",
            "#Layers: 9 #Units: 77 LR: 0.001 Training loss: 0.6933416724205017 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 92 LR: 0.001 Epoch 50: train loss: 0.6930844783782959\n",
            "#Layers: 9 #Units: 92 LR: 0.001 Training loss: 0.6930844783782959 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.6981978416442871\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Training loss: 0.6981978416442871 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.6150293946266174\n",
            "#Layers: 2 #Units: 17 LR: 0.01 Training loss: 0.6150293946266174 Cross validation accuracy: 0.8047337278106509\n",
            "#Layers: 2 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.652118444442749\n",
            "#Layers: 2 #Units: 32 LR: 0.01 Training loss: 0.652118444442749 Cross validation accuracy: 0.7218934911242604\n",
            "#Layers: 2 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.567703127861023\n",
            "#Layers: 2 #Units: 47 LR: 0.01 Training loss: 0.567703127861023 Cross validation accuracy: 0.8520710059171598\n",
            "#Layers: 2 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.5370369553565979\n",
            "#Layers: 2 #Units: 62 LR: 0.01 Training loss: 0.5370369553565979 Cross validation accuracy: 0.8698224852071006\n",
            "#Layers: 2 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.5538334250450134\n",
            "#Layers: 2 #Units: 77 LR: 0.01 Training loss: 0.5538334250450134 Cross validation accuracy: 0.863905325443787\n",
            "#Layers: 2 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.5323624014854431\n",
            "#Layers: 2 #Units: 92 LR: 0.01 Training loss: 0.5323624014854431 Cross validation accuracy: 0.8698224852071006\n",
            "#Layers: 3 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.7367758750915527\n",
            "#Layers: 3 #Units: 2 LR: 0.01 Training loss: 0.7367758750915527 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.675482988357544\n",
            "#Layers: 3 #Units: 17 LR: 0.01 Training loss: 0.675482988357544 Cross validation accuracy: 0.7100591715976331\n",
            "#Layers: 3 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6596463322639465\n",
            "#Layers: 3 #Units: 32 LR: 0.01 Training loss: 0.6596463322639465 Cross validation accuracy: 0.7396449704142012\n",
            "#Layers: 3 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.6828747391700745\n",
            "#Layers: 3 #Units: 47 LR: 0.01 Training loss: 0.6828747391700745 Cross validation accuracy: 0.6094674556213018\n",
            "#Layers: 3 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.6571159362792969\n",
            "#Layers: 3 #Units: 62 LR: 0.01 Training loss: 0.6571159362792969 Cross validation accuracy: 0.8994082840236687\n",
            "#Layers: 3 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6235933899879456\n",
            "#Layers: 3 #Units: 77 LR: 0.01 Training loss: 0.6235933899879456 Cross validation accuracy: 0.9053254437869822\n",
            "#Layers: 3 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6436660289764404\n",
            "#Layers: 3 #Units: 92 LR: 0.01 Training loss: 0.6436660289764404 Cross validation accuracy: 0.7751479289940828\n",
            "#Layers: 4 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.695841908454895\n",
            "#Layers: 4 #Units: 2 LR: 0.01 Training loss: 0.695841908454895 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.6900913715362549\n",
            "#Layers: 4 #Units: 17 LR: 0.01 Training loss: 0.6900913715362549 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6932446360588074\n",
            "#Layers: 4 #Units: 32 LR: 0.01 Training loss: 0.6932446360588074 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.6907115578651428\n",
            "#Layers: 4 #Units: 47 LR: 0.01 Training loss: 0.6907115578651428 Cross validation accuracy: 0.8165680473372781\n",
            "#Layers: 4 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.677893340587616\n",
            "#Layers: 4 #Units: 62 LR: 0.01 Training loss: 0.677893340587616 Cross validation accuracy: 0.5325443786982249\n",
            "#Layers: 4 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6813423037528992\n",
            "#Layers: 4 #Units: 77 LR: 0.01 Training loss: 0.6813423037528992 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6799678206443787\n",
            "#Layers: 4 #Units: 92 LR: 0.01 Training loss: 0.6799678206443787 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.7020249962806702\n",
            "#Layers: 5 #Units: 2 LR: 0.01 Training loss: 0.7020249962806702 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.693389356136322\n",
            "#Layers: 5 #Units: 17 LR: 0.01 Training loss: 0.693389356136322 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6934617161750793\n",
            "#Layers: 5 #Units: 32 LR: 0.01 Training loss: 0.6934617161750793 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.6954253911972046\n",
            "#Layers: 5 #Units: 47 LR: 0.01 Training loss: 0.6954253911972046 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.6920621991157532\n",
            "#Layers: 5 #Units: 62 LR: 0.01 Training loss: 0.6920621991157532 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6911711692810059\n",
            "#Layers: 5 #Units: 77 LR: 0.01 Training loss: 0.6911711692810059 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6881535053253174\n",
            "#Layers: 5 #Units: 92 LR: 0.01 Training loss: 0.6881535053253174 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.6944229602813721\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Training loss: 0.6944229602813721 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.695223331451416\n",
            "#Layers: 6 #Units: 17 LR: 0.01 Training loss: 0.695223331451416 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6923699378967285\n",
            "#Layers: 6 #Units: 32 LR: 0.01 Training loss: 0.6923699378967285 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.69221031665802\n",
            "#Layers: 6 #Units: 47 LR: 0.01 Training loss: 0.69221031665802 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.6912187933921814\n",
            "#Layers: 6 #Units: 62 LR: 0.01 Training loss: 0.6912187933921814 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6921308636665344\n",
            "#Layers: 6 #Units: 77 LR: 0.01 Training loss: 0.6921308636665344 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6936447620391846\n",
            "#Layers: 6 #Units: 92 LR: 0.01 Training loss: 0.6936447620391846 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.7099524140357971\n",
            "#Layers: 7 #Units: 2 LR: 0.01 Training loss: 0.7099524140357971 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.6951111555099487\n",
            "#Layers: 7 #Units: 17 LR: 0.01 Training loss: 0.6951111555099487 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6981926560401917\n",
            "#Layers: 7 #Units: 32 LR: 0.01 Training loss: 0.6981926560401917 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.6956717371940613\n",
            "#Layers: 7 #Units: 47 LR: 0.01 Training loss: 0.6956717371940613 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.6935360431671143\n",
            "#Layers: 7 #Units: 62 LR: 0.01 Training loss: 0.6935360431671143 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6931256055831909\n",
            "#Layers: 7 #Units: 77 LR: 0.01 Training loss: 0.6931256055831909 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6929531097412109\n",
            "#Layers: 7 #Units: 92 LR: 0.01 Training loss: 0.6929531097412109 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.7184138894081116\n",
            "#Layers: 8 #Units: 2 LR: 0.01 Training loss: 0.7184138894081116 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.6960199475288391\n",
            "#Layers: 8 #Units: 17 LR: 0.01 Training loss: 0.6960199475288391 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6940335035324097\n",
            "#Layers: 8 #Units: 32 LR: 0.01 Training loss: 0.6940335035324097 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.6937081217765808\n",
            "#Layers: 8 #Units: 47 LR: 0.01 Training loss: 0.6937081217765808 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.6935899257659912\n",
            "#Layers: 8 #Units: 62 LR: 0.01 Training loss: 0.6935899257659912 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6931394338607788\n",
            "#Layers: 8 #Units: 77 LR: 0.01 Training loss: 0.6931394338607788 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6930857300758362\n",
            "#Layers: 8 #Units: 92 LR: 0.01 Training loss: 0.6930857300758362 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 2 LR: 0.01 Epoch 50: train loss: 0.7155553698539734\n",
            "#Layers: 9 #Units: 2 LR: 0.01 Training loss: 0.7155553698539734 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 17 LR: 0.01 Epoch 50: train loss: 0.6938391327857971\n",
            "#Layers: 9 #Units: 17 LR: 0.01 Training loss: 0.6938391327857971 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 32 LR: 0.01 Epoch 50: train loss: 0.6939921379089355\n",
            "#Layers: 9 #Units: 32 LR: 0.01 Training loss: 0.6939921379089355 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 47 LR: 0.01 Epoch 50: train loss: 0.6934341192245483\n",
            "#Layers: 9 #Units: 47 LR: 0.01 Training loss: 0.6934341192245483 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 62 LR: 0.01 Epoch 50: train loss: 0.693111777305603\n",
            "#Layers: 9 #Units: 62 LR: 0.01 Training loss: 0.693111777305603 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 77 LR: 0.01 Epoch 50: train loss: 0.6935223340988159\n",
            "#Layers: 9 #Units: 77 LR: 0.01 Training loss: 0.6935223340988159 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 92 LR: 0.01 Epoch 50: train loss: 0.6931169033050537\n",
            "#Layers: 9 #Units: 92 LR: 0.01 Training loss: 0.6931169033050537 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6392810344696045\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Training loss: 0.6392810344696045 Cross validation accuracy: 0.6982248520710059\n",
            "#Layers: 2 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.4839014410972595\n",
            "#Layers: 2 #Units: 17 LR: 0.1 Training loss: 0.4839014410972595 Cross validation accuracy: 0.8698224852071006\n",
            "#Layers: 2 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.41024050116539\n",
            "#Layers: 2 #Units: 32 LR: 0.1 Training loss: 0.41024050116539 Cross validation accuracy: 0.8698224852071006\n",
            "#Layers: 2 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.3449229896068573\n",
            "#Layers: 2 #Units: 47 LR: 0.1 Training loss: 0.3449229896068573 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 2 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.3381313681602478\n",
            "#Layers: 2 #Units: 62 LR: 0.1 Training loss: 0.3381313681602478 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 2 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.32682183384895325\n",
            "#Layers: 2 #Units: 77 LR: 0.1 Training loss: 0.32682183384895325 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 2 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.30839163064956665\n",
            "#Layers: 2 #Units: 92 LR: 0.1 Training loss: 0.30839163064956665 Cross validation accuracy: 0.8994082840236687\n",
            "#Layers: 3 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6950432658195496\n",
            "#Layers: 3 #Units: 2 LR: 0.1 Training loss: 0.6950432658195496 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.40818119049072266\n",
            "#Layers: 3 #Units: 17 LR: 0.1 Training loss: 0.40818119049072266 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 3 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.41253629326820374\n",
            "#Layers: 3 #Units: 32 LR: 0.1 Training loss: 0.41253629326820374 Cross validation accuracy: 0.863905325443787\n",
            "#Layers: 3 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.3599705100059509\n",
            "#Layers: 3 #Units: 47 LR: 0.1 Training loss: 0.3599705100059509 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 3 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.3566516041755676\n",
            "#Layers: 3 #Units: 62 LR: 0.1 Training loss: 0.3566516041755676 Cross validation accuracy: 0.8757396449704142\n",
            "#Layers: 3 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.3427368104457855\n",
            "#Layers: 3 #Units: 77 LR: 0.1 Training loss: 0.3427368104457855 Cross validation accuracy: 0.8757396449704142\n",
            "#Layers: 3 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.32831037044525146\n",
            "#Layers: 3 #Units: 92 LR: 0.1 Training loss: 0.32831037044525146 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 4 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6425511837005615\n",
            "#Layers: 4 #Units: 2 LR: 0.1 Training loss: 0.6425511837005615 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.6085371375083923\n",
            "#Layers: 4 #Units: 17 LR: 0.1 Training loss: 0.6085371375083923 Cross validation accuracy: 0.8520710059171598\n",
            "#Layers: 4 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.5887818932533264\n",
            "#Layers: 4 #Units: 32 LR: 0.1 Training loss: 0.5887818932533264 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 4 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.524512767791748\n",
            "#Layers: 4 #Units: 47 LR: 0.1 Training loss: 0.524512767791748 Cross validation accuracy: 0.8757396449704142\n",
            "#Layers: 4 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.4836561977863312\n",
            "#Layers: 4 #Units: 62 LR: 0.1 Training loss: 0.4836561977863312 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 4 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.3846277892589569\n",
            "#Layers: 4 #Units: 77 LR: 0.1 Training loss: 0.3846277892589569 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 4 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.3641558885574341\n",
            "#Layers: 4 #Units: 92 LR: 0.1 Training loss: 0.3641558885574341 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 5 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6930233836174011\n",
            "#Layers: 5 #Units: 2 LR: 0.1 Training loss: 0.6930233836174011 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.6807389855384827\n",
            "#Layers: 5 #Units: 17 LR: 0.1 Training loss: 0.6807389855384827 Cross validation accuracy: 0.727810650887574\n",
            "#Layers: 5 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.688162624835968\n",
            "#Layers: 5 #Units: 32 LR: 0.1 Training loss: 0.688162624835968 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.6552810072898865\n",
            "#Layers: 5 #Units: 47 LR: 0.1 Training loss: 0.6552810072898865 Cross validation accuracy: 0.8579881656804734\n",
            "#Layers: 5 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.6687636971473694\n",
            "#Layers: 5 #Units: 62 LR: 0.1 Training loss: 0.6687636971473694 Cross validation accuracy: 0.8402366863905325\n",
            "#Layers: 5 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.5720930695533752\n",
            "#Layers: 5 #Units: 77 LR: 0.1 Training loss: 0.5720930695533752 Cross validation accuracy: 0.8698224852071006\n",
            "#Layers: 5 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.6188551187515259\n",
            "#Layers: 5 #Units: 92 LR: 0.1 Training loss: 0.6188551187515259 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6930307745933533\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Training loss: 0.6930307745933533 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.690153181552887\n",
            "#Layers: 6 #Units: 17 LR: 0.1 Training loss: 0.690153181552887 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.6890938878059387\n",
            "#Layers: 6 #Units: 32 LR: 0.1 Training loss: 0.6890938878059387 Cross validation accuracy: 0.7218934911242604\n",
            "#Layers: 6 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.6880996823310852\n",
            "#Layers: 6 #Units: 47 LR: 0.1 Training loss: 0.6880996823310852 Cross validation accuracy: 0.7692307692307693\n",
            "#Layers: 6 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.6810216307640076\n",
            "#Layers: 6 #Units: 62 LR: 0.1 Training loss: 0.6810216307640076 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.6815508604049683\n",
            "#Layers: 6 #Units: 77 LR: 0.1 Training loss: 0.6815508604049683 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.6849305629730225\n",
            "#Layers: 6 #Units: 92 LR: 0.1 Training loss: 0.6849305629730225 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6935567259788513\n",
            "#Layers: 7 #Units: 2 LR: 0.1 Training loss: 0.6935567259788513 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.6922635436058044\n",
            "#Layers: 7 #Units: 17 LR: 0.1 Training loss: 0.6922635436058044 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.6921280026435852\n",
            "#Layers: 7 #Units: 32 LR: 0.1 Training loss: 0.6921280026435852 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.6900779008865356\n",
            "#Layers: 7 #Units: 47 LR: 0.1 Training loss: 0.6900779008865356 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.6924862861633301\n",
            "#Layers: 7 #Units: 62 LR: 0.1 Training loss: 0.6924862861633301 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.6923834681510925\n",
            "#Layers: 7 #Units: 77 LR: 0.1 Training loss: 0.6923834681510925 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.6909950971603394\n",
            "#Layers: 7 #Units: 92 LR: 0.1 Training loss: 0.6909950971603394 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6930251717567444\n",
            "#Layers: 8 #Units: 2 LR: 0.1 Training loss: 0.6930251717567444 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.6929488182067871\n",
            "#Layers: 8 #Units: 17 LR: 0.1 Training loss: 0.6929488182067871 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.6930475234985352\n",
            "#Layers: 8 #Units: 32 LR: 0.1 Training loss: 0.6930475234985352 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.6930800080299377\n",
            "#Layers: 8 #Units: 47 LR: 0.1 Training loss: 0.6930800080299377 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.6930193305015564\n",
            "#Layers: 8 #Units: 62 LR: 0.1 Training loss: 0.6930193305015564 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.6929091811180115\n",
            "#Layers: 8 #Units: 77 LR: 0.1 Training loss: 0.6929091811180115 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.6920175552368164\n",
            "#Layers: 8 #Units: 92 LR: 0.1 Training loss: 0.6920175552368164 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 2 LR: 0.1 Epoch 50: train loss: 0.6930223703384399\n",
            "#Layers: 9 #Units: 2 LR: 0.1 Training loss: 0.6930223703384399 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 17 LR: 0.1 Epoch 50: train loss: 0.6931060552597046\n",
            "#Layers: 9 #Units: 17 LR: 0.1 Training loss: 0.6931060552597046 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 32 LR: 0.1 Epoch 50: train loss: 0.692823588848114\n",
            "#Layers: 9 #Units: 32 LR: 0.1 Training loss: 0.692823588848114 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 47 LR: 0.1 Epoch 50: train loss: 0.6930090188980103\n",
            "#Layers: 9 #Units: 47 LR: 0.1 Training loss: 0.6930090188980103 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 62 LR: 0.1 Epoch 50: train loss: 0.6931346654891968\n",
            "#Layers: 9 #Units: 62 LR: 0.1 Training loss: 0.6931346654891968 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 77 LR: 0.1 Epoch 50: train loss: 0.6929917335510254\n",
            "#Layers: 9 #Units: 77 LR: 0.1 Training loss: 0.6929917335510254 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 92 LR: 0.1 Epoch 50: train loss: 0.6930196285247803\n",
            "#Layers: 9 #Units: 92 LR: 0.1 Training loss: 0.6930196285247803 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 50: train loss: 0.28798606991767883\n",
            "#Layers: 2 #Units: 2 LR: 1 Training loss: 0.28798606991767883 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 2 #Units: 17 LR: 1 Epoch 50: train loss: 0.2415439486503601\n",
            "#Layers: 2 #Units: 17 LR: 1 Training loss: 0.2415439486503601 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 2 #Units: 32 LR: 1 Epoch 50: train loss: 0.23994585871696472\n",
            "#Layers: 2 #Units: 32 LR: 1 Training loss: 0.23994585871696472 Cross validation accuracy: 0.8875739644970414\n",
            "#Layers: 2 #Units: 47 LR: 1 Epoch 50: train loss: 0.20789875090122223\n",
            "#Layers: 2 #Units: 47 LR: 1 Training loss: 0.20789875090122223 Cross validation accuracy: 0.9230769230769231\n",
            "#Layers: 2 #Units: 62 LR: 1 Epoch 50: train loss: 0.1907251924276352\n",
            "#Layers: 2 #Units: 62 LR: 1 Training loss: 0.1907251924276352 Cross validation accuracy: 0.9289940828402367\n",
            "#Layers: 2 #Units: 77 LR: 1 Epoch 50: train loss: 0.20742109417915344\n",
            "#Layers: 2 #Units: 77 LR: 1 Training loss: 0.20742109417915344 Cross validation accuracy: 0.9289940828402367\n",
            "#Layers: 2 #Units: 92 LR: 1 Epoch 50: train loss: 0.19951537251472473\n",
            "#Layers: 2 #Units: 92 LR: 1 Training loss: 0.19951537251472473 Cross validation accuracy: 0.9112426035502958\n",
            "#Layers: 3 #Units: 2 LR: 1 Epoch 50: train loss: 0.3480360805988312\n",
            "#Layers: 3 #Units: 2 LR: 1 Training loss: 0.3480360805988312 Cross validation accuracy: 0.9053254437869822\n",
            "#Layers: 3 #Units: 17 LR: 1 Epoch 50: train loss: 0.32647502422332764\n",
            "#Layers: 3 #Units: 17 LR: 1 Training loss: 0.32647502422332764 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 3 #Units: 32 LR: 1 Epoch 50: train loss: 0.16714756190776825\n",
            "#Layers: 3 #Units: 32 LR: 1 Training loss: 0.16714756190776825 Cross validation accuracy: 0.9526627218934911\n",
            "#Layers: 3 #Units: 47 LR: 1 Epoch 50: train loss: 0.1355610191822052\n",
            "#Layers: 3 #Units: 47 LR: 1 Training loss: 0.1355610191822052 Cross validation accuracy: 0.9585798816568047\n",
            "#Layers: 3 #Units: 62 LR: 1 Epoch 50: train loss: 0.12823976576328278\n",
            "#Layers: 3 #Units: 62 LR: 1 Training loss: 0.12823976576328278 Cross validation accuracy: 0.9585798816568047\n",
            "#Layers: 3 #Units: 77 LR: 1 Epoch 50: train loss: 0.10805312544107437\n",
            "#Layers: 3 #Units: 77 LR: 1 Training loss: 0.10805312544107437 Cross validation accuracy: 0.9881656804733728\n",
            "#Layers: 3 #Units: 92 LR: 1 Epoch 50: train loss: 0.19785766303539276\n",
            "#Layers: 3 #Units: 92 LR: 1 Training loss: 0.19785766303539276 Cross validation accuracy: 0.9171597633136095\n",
            "#Layers: 4 #Units: 2 LR: 1 Epoch 50: train loss: 0.6927814483642578\n",
            "#Layers: 4 #Units: 2 LR: 1 Training loss: 0.6927814483642578 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 17 LR: 1 Epoch 50: train loss: 0.20583300292491913\n",
            "#Layers: 4 #Units: 17 LR: 1 Training loss: 0.20583300292491913 Cross validation accuracy: 0.9171597633136095\n",
            "#Layers: 4 #Units: 32 LR: 1 Epoch 50: train loss: 0.26002150774002075\n",
            "#Layers: 4 #Units: 32 LR: 1 Training loss: 0.26002150774002075 Cross validation accuracy: 0.8816568047337278\n",
            "#Layers: 4 #Units: 47 LR: 1 Epoch 50: train loss: 0.27953726053237915\n",
            "#Layers: 4 #Units: 47 LR: 1 Training loss: 0.27953726053237915 Cross validation accuracy: 0.9526627218934911\n",
            "#Layers: 4 #Units: 62 LR: 1 Epoch 50: train loss: 0.23746392130851746\n",
            "#Layers: 4 #Units: 62 LR: 1 Training loss: 0.23746392130851746 Cross validation accuracy: 0.9289940828402367\n",
            "#Layers: 4 #Units: 77 LR: 1 Epoch 50: train loss: 0.15836861729621887\n",
            "#Layers: 4 #Units: 77 LR: 1 Training loss: 0.15836861729621887 Cross validation accuracy: 0.9467455621301775\n",
            "#Layers: 4 #Units: 92 LR: 1 Epoch 50: train loss: 0.18221193552017212\n",
            "#Layers: 4 #Units: 92 LR: 1 Training loss: 0.18221193552017212 Cross validation accuracy: 0.9585798816568047\n",
            "#Layers: 5 #Units: 2 LR: 1 Epoch 50: train loss: 0.48168250918388367\n",
            "#Layers: 5 #Units: 2 LR: 1 Training loss: 0.48168250918388367 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 5 #Units: 17 LR: 1 Epoch 50: train loss: 0.3118683993816376\n",
            "#Layers: 5 #Units: 17 LR: 1 Training loss: 0.3118683993816376 Cross validation accuracy: 0.9289940828402367\n",
            "#Layers: 5 #Units: 32 LR: 1 Epoch 50: train loss: 0.20208892226219177\n",
            "#Layers: 5 #Units: 32 LR: 1 Training loss: 0.20208892226219177 Cross validation accuracy: 0.9526627218934911\n",
            "#Layers: 5 #Units: 47 LR: 1 Epoch 50: train loss: 0.3235720098018646\n",
            "#Layers: 5 #Units: 47 LR: 1 Training loss: 0.3235720098018646 Cross validation accuracy: 0.7751479289940828\n",
            "#Layers: 5 #Units: 62 LR: 1 Epoch 50: train loss: 0.2285550981760025\n",
            "#Layers: 5 #Units: 62 LR: 1 Training loss: 0.2285550981760025 Cross validation accuracy: 0.9053254437869822\n",
            "#Layers: 5 #Units: 77 LR: 1 Epoch 50: train loss: 0.5961349606513977\n",
            "#Layers: 5 #Units: 77 LR: 1 Training loss: 0.5961349606513977 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 92 LR: 1 Epoch 50: train loss: 0.15134954452514648\n",
            "#Layers: 5 #Units: 92 LR: 1 Training loss: 0.15134954452514648 Cross validation accuracy: 0.9230769230769231\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 50: train loss: 0.6930218935012817\n",
            "#Layers: 6 #Units: 2 LR: 1 Training loss: 0.6930218935012817 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 1 Epoch 50: train loss: 0.3494541347026825\n",
            "#Layers: 6 #Units: 17 LR: 1 Training loss: 0.3494541347026825 Cross validation accuracy: 0.8994082840236687\n",
            "#Layers: 6 #Units: 32 LR: 1 Epoch 50: train loss: 0.21451734006404877\n",
            "#Layers: 6 #Units: 32 LR: 1 Training loss: 0.21451734006404877 Cross validation accuracy: 0.9171597633136095\n",
            "#Layers: 6 #Units: 47 LR: 1 Epoch 50: train loss: 0.19012606143951416\n",
            "#Layers: 6 #Units: 47 LR: 1 Training loss: 0.19012606143951416 Cross validation accuracy: 0.9289940828402367\n",
            "#Layers: 6 #Units: 62 LR: 1 Epoch 50: train loss: 0.24460117518901825\n",
            "#Layers: 6 #Units: 62 LR: 1 Training loss: 0.24460117518901825 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 6 #Units: 77 LR: 1 Epoch 50: train loss: 0.229684516787529\n",
            "#Layers: 6 #Units: 77 LR: 1 Training loss: 0.229684516787529 Cross validation accuracy: 0.8994082840236687\n",
            "#Layers: 6 #Units: 92 LR: 1 Epoch 50: train loss: 0.20074184238910675\n",
            "#Layers: 6 #Units: 92 LR: 1 Training loss: 0.20074184238910675 Cross validation accuracy: 0.9171597633136095\n",
            "#Layers: 7 #Units: 2 LR: 1 Epoch 50: train loss: 0.6930218935012817\n",
            "#Layers: 7 #Units: 2 LR: 1 Training loss: 0.6930218935012817 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 17 LR: 1 Epoch 50: train loss: 0.6914523839950562\n",
            "#Layers: 7 #Units: 17 LR: 1 Training loss: 0.6914523839950562 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 32 LR: 1 Epoch 50: train loss: 0.4464109539985657\n",
            "#Layers: 7 #Units: 32 LR: 1 Training loss: 0.4464109539985657 Cross validation accuracy: 0.8284023668639053\n",
            "#Layers: 7 #Units: 47 LR: 1 Epoch 50: train loss: 0.35089579224586487\n",
            "#Layers: 7 #Units: 47 LR: 1 Training loss: 0.35089579224586487 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 7 #Units: 62 LR: 1 Epoch 50: train loss: 0.5534327626228333\n",
            "#Layers: 7 #Units: 62 LR: 1 Training loss: 0.5534327626228333 Cross validation accuracy: 0.5502958579881657\n",
            "#Layers: 7 #Units: 77 LR: 1 Epoch 50: train loss: 0.23156578838825226\n",
            "#Layers: 7 #Units: 77 LR: 1 Training loss: 0.23156578838825226 Cross validation accuracy: 0.9053254437869822\n",
            "#Layers: 7 #Units: 92 LR: 1 Epoch 50: train loss: 0.4287742078304291\n",
            "#Layers: 7 #Units: 92 LR: 1 Training loss: 0.4287742078304291 Cross validation accuracy: 0.8224852071005917\n",
            "#Layers: 8 #Units: 2 LR: 1 Epoch 50: train loss: 0.6930218935012817\n",
            "#Layers: 8 #Units: 2 LR: 1 Training loss: 0.6930218935012817 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 17 LR: 1 Epoch 50: train loss: 0.6924622058868408\n",
            "#Layers: 8 #Units: 17 LR: 1 Training loss: 0.6924622058868408 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 32 LR: 1 Epoch 50: train loss: 0.6904425024986267\n",
            "#Layers: 8 #Units: 32 LR: 1 Training loss: 0.6904425024986267 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 47 LR: 1 Epoch 50: train loss: 0.6923074126243591\n",
            "#Layers: 8 #Units: 47 LR: 1 Training loss: 0.6923074126243591 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 62 LR: 1 Epoch 50: train loss: 0.7793087363243103\n",
            "#Layers: 8 #Units: 62 LR: 1 Training loss: 0.7793087363243103 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 77 LR: 1 Epoch 50: train loss: 0.4179301857948303\n",
            "#Layers: 8 #Units: 77 LR: 1 Training loss: 0.4179301857948303 Cross validation accuracy: 0.8994082840236687\n",
            "#Layers: 8 #Units: 92 LR: 1 Epoch 50: train loss: 0.6540743708610535\n",
            "#Layers: 8 #Units: 92 LR: 1 Training loss: 0.6540743708610535 Cross validation accuracy: 0.834319526627219\n",
            "#Layers: 9 #Units: 2 LR: 1 Epoch 50: train loss: 0.6930218935012817\n",
            "#Layers: 9 #Units: 2 LR: 1 Training loss: 0.6930218935012817 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 17 LR: 1 Epoch 50: train loss: 0.6927534341812134\n",
            "#Layers: 9 #Units: 17 LR: 1 Training loss: 0.6927534341812134 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 32 LR: 1 Epoch 50: train loss: 0.6928956508636475\n",
            "#Layers: 9 #Units: 32 LR: 1 Training loss: 0.6928956508636475 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 47 LR: 1 Epoch 50: train loss: 0.6913889646530151\n",
            "#Layers: 9 #Units: 47 LR: 1 Training loss: 0.6913889646530151 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 62 LR: 1 Epoch 50: train loss: 0.692039430141449\n",
            "#Layers: 9 #Units: 62 LR: 1 Training loss: 0.692039430141449 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 77 LR: 1 Epoch 50: train loss: 0.6918905377388\n",
            "#Layers: 9 #Units: 77 LR: 1 Training loss: 0.6918905377388 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 92 LR: 1 Epoch 50: train loss: 0.6921721696853638\n",
            "#Layers: 9 #Units: 92 LR: 1 Training loss: 0.6921721696853638 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 2 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 2 #Units: 2 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 17 LR: 10 Epoch 50: train loss: 0.26223233342170715\n",
            "#Layers: 2 #Units: 17 LR: 10 Training loss: 0.26223233342170715 Cross validation accuracy: 0.893491124260355\n",
            "#Layers: 2 #Units: 32 LR: 10 Epoch 50: train loss: 0.22351256012916565\n",
            "#Layers: 2 #Units: 32 LR: 10 Training loss: 0.22351256012916565 Cross validation accuracy: 0.9467455621301775\n",
            "#Layers: 2 #Units: 47 LR: 10 Epoch 50: train loss: 0.2776147127151489\n",
            "#Layers: 2 #Units: 47 LR: 10 Training loss: 0.2776147127151489 Cross validation accuracy: 0.8816568047337278\n",
            "#Layers: 2 #Units: 62 LR: 10 Epoch 50: train loss: 0.2344093918800354\n",
            "#Layers: 2 #Units: 62 LR: 10 Training loss: 0.2344093918800354 Cross validation accuracy: 0.8757396449704142\n",
            "#Layers: 2 #Units: 77 LR: 10 Epoch 50: train loss: 0.18127332627773285\n",
            "#Layers: 2 #Units: 77 LR: 10 Training loss: 0.18127332627773285 Cross validation accuracy: 0.9171597633136095\n",
            "#Layers: 2 #Units: 92 LR: 10 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 2 #Units: 92 LR: 10 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 2 LR: 10 Epoch 50: train loss: 1.0347501039505005\n",
            "#Layers: 3 #Units: 2 LR: 10 Training loss: 1.0347501039505005 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 17 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 3 #Units: 17 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 32 LR: 10 Epoch 50: train loss: 0.6851619482040405\n",
            "#Layers: 3 #Units: 32 LR: 10 Training loss: 0.6851619482040405 Cross validation accuracy: 0.863905325443787\n",
            "#Layers: 3 #Units: 47 LR: 10 Epoch 50: train loss: 20.750988006591797\n",
            "#Layers: 3 #Units: 47 LR: 10 Training loss: 20.750988006591797 Cross validation accuracy: 0.8047337278106509\n",
            "#Layers: 3 #Units: 62 LR: 10 Epoch 50: train loss: 37.98133850097656\n",
            "#Layers: 3 #Units: 62 LR: 10 Training loss: 37.98133850097656 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 77 LR: 10 Epoch 50: train loss: 45.54623794555664\n",
            "#Layers: 3 #Units: 77 LR: 10 Training loss: 45.54623794555664 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 92 LR: 10 Epoch 50: train loss: 47.3750114440918\n",
            "#Layers: 3 #Units: 92 LR: 10 Training loss: 47.3750114440918 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 2 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 4 #Units: 2 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 17 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 4 #Units: 17 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 32 LR: 10 Epoch 50: train loss: 50.66450119018555\n",
            "#Layers: 4 #Units: 32 LR: 10 Training loss: 50.66450119018555 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 47 LR: 10 Epoch 50: train loss: 47.91043472290039\n",
            "#Layers: 4 #Units: 47 LR: 10 Training loss: 47.91043472290039 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 62 LR: 10 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 4 #Units: 62 LR: 10 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 77 LR: 10 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 4 #Units: 77 LR: 10 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 92 LR: 10 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 4 #Units: 92 LR: 10 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 2 LR: 10 Epoch 50: train loss: 1.034749984741211\n",
            "#Layers: 5 #Units: 2 LR: 10 Training loss: 1.034749984741211 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 17 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 5 #Units: 17 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 32 LR: 10 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 5 #Units: 32 LR: 10 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 47 LR: 10 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 5 #Units: 47 LR: 10 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 62 LR: 10 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 5 #Units: 62 LR: 10 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 77 LR: 10 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 5 #Units: 77 LR: 10 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 92 LR: 10 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 5 #Units: 92 LR: 10 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 2 LR: 10 Epoch 50: train loss: 1.0347501039505005\n",
            "#Layers: 6 #Units: 2 LR: 10 Training loss: 1.0347501039505005 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 10 Epoch 50: train loss: 1.034749984741211\n",
            "#Layers: 6 #Units: 17 LR: 10 Training loss: 1.034749984741211 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 32 LR: 10 Epoch 50: train loss: 1.034749984741211\n",
            "#Layers: 6 #Units: 32 LR: 10 Training loss: 1.034749984741211 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 47 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 6 #Units: 47 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 62 LR: 10 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 6 #Units: 62 LR: 10 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 77 LR: 10 Epoch 50: train loss: 41.53520965576172\n",
            "#Layers: 6 #Units: 77 LR: 10 Training loss: 41.53520965576172 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 92 LR: 10 Epoch 50: train loss: 1.034749984741211\n",
            "#Layers: 6 #Units: 92 LR: 10 Training loss: 1.034749984741211 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 2 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 7 #Units: 2 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 17 LR: 10 Epoch 50: train loss: 1.0347501039505005\n",
            "#Layers: 7 #Units: 17 LR: 10 Training loss: 1.0347501039505005 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 32 LR: 10 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 7 #Units: 32 LR: 10 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 47 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 7 #Units: 47 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 62 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 7 #Units: 62 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 77 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 7 #Units: 77 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 92 LR: 10 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 7 #Units: 92 LR: 10 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 2 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 8 #Units: 2 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 17 LR: 10 Epoch 50: train loss: 1.034749984741211\n",
            "#Layers: 8 #Units: 17 LR: 10 Training loss: 1.034749984741211 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 32 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 8 #Units: 32 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 47 LR: 10 Epoch 50: train loss: 1.034749984741211\n",
            "#Layers: 8 #Units: 47 LR: 10 Training loss: 1.034749984741211 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 62 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 8 #Units: 62 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 77 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 8 #Units: 77 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 92 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 8 #Units: 92 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 2 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 9 #Units: 2 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 17 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 9 #Units: 17 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 32 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 9 #Units: 32 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 47 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 9 #Units: 47 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 62 LR: 10 Epoch 50: train loss: 1.051932692527771\n",
            "#Layers: 9 #Units: 62 LR: 10 Training loss: 1.051932692527771 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 77 LR: 10 Epoch 50: train loss: 1.0347501039505005\n",
            "#Layers: 9 #Units: 77 LR: 10 Training loss: 1.0347501039505005 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 92 LR: 10 Epoch 50: train loss: 1.0347501039505005\n",
            "#Layers: 9 #Units: 92 LR: 10 Training loss: 1.0347501039505005 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 2 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 2 #Units: 2 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 17 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 2 #Units: 17 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 32 LR: 100 Epoch 50: train loss: 19.880022048950195\n",
            "#Layers: 2 #Units: 32 LR: 100 Training loss: 19.880022048950195 Cross validation accuracy: 0.8402366863905325\n",
            "#Layers: 2 #Units: 47 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 2 #Units: 47 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 62 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 2 #Units: 62 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 2 #Units: 77 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 2 #Units: 77 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 2 #Units: 92 LR: 100 Epoch 50: train loss: 49.60474395751953\n",
            "#Layers: 2 #Units: 92 LR: 100 Training loss: 49.60474395751953 Cross validation accuracy: 0.514792899408284\n",
            "#Layers: 3 #Units: 2 LR: 100 Epoch 50: train loss: 22.05055809020996\n",
            "#Layers: 3 #Units: 2 LR: 100 Training loss: 22.05055809020996 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 17 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 3 #Units: 17 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 32 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 3 #Units: 32 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 47 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 3 #Units: 47 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 62 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 3 #Units: 62 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 3 #Units: 77 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 3 #Units: 77 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 3 #Units: 92 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 3 #Units: 92 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 2 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 4 #Units: 2 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 4 #Units: 17 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 4 #Units: 17 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 32 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 4 #Units: 32 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 47 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 4 #Units: 47 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 62 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 4 #Units: 62 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 77 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 4 #Units: 77 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 4 #Units: 92 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 4 #Units: 92 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 2 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 5 #Units: 2 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 17 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 5 #Units: 17 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 32 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 5 #Units: 32 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 47 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 5 #Units: 47 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 62 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 5 #Units: 62 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 5 #Units: 77 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 5 #Units: 77 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 5 #Units: 92 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 5 #Units: 92 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 2 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 6 #Units: 2 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 17 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 6 #Units: 17 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 6 #Units: 32 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 6 #Units: 32 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 47 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 6 #Units: 47 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 62 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 6 #Units: 62 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 77 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 6 #Units: 77 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 6 #Units: 92 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 6 #Units: 92 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 2 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 7 #Units: 2 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 17 LR: 100 Epoch 50: train loss: 23.581056594848633\n",
            "#Layers: 7 #Units: 17 LR: 100 Training loss: 23.581056594848633 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 32 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 7 #Units: 32 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 47 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 7 #Units: 47 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 7 #Units: 62 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 7 #Units: 62 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 77 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 7 #Units: 77 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 7 #Units: 92 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 7 #Units: 92 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 2 LR: 100 Epoch 50: train loss: 20.340524673461914\n",
            "#Layers: 8 #Units: 2 LR: 100 Training loss: 20.340524673461914 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 17 LR: 100 Epoch 50: train loss: 22.32073211669922\n",
            "#Layers: 8 #Units: 17 LR: 100 Training loss: 22.32073211669922 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 32 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 8 #Units: 32 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 47 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 8 #Units: 47 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 62 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 8 #Units: 62 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 8 #Units: 77 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 8 #Units: 77 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 8 #Units: 92 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 8 #Units: 92 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 2 LR: 100 Epoch 50: train loss: 21.215967178344727\n",
            "#Layers: 9 #Units: 2 LR: 100 Training loss: 21.215967178344727 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 17 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 9 #Units: 17 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 32 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 9 #Units: 32 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 47 LR: 100 Epoch 50: train loss: 49.20948791503906\n",
            "#Layers: 9 #Units: 47 LR: 100 Training loss: 49.20948791503906 Cross validation accuracy: 0.5266272189349113\n",
            "#Layers: 9 #Units: 62 LR: 100 Epoch 50: train loss: 22.374794006347656\n",
            "#Layers: 9 #Units: 62 LR: 100 Training loss: 22.374794006347656 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 77 LR: 100 Epoch 50: train loss: 23.559160232543945\n",
            "#Layers: 9 #Units: 77 LR: 100 Training loss: 23.559160232543945 Cross validation accuracy: 0.47337278106508873\n",
            "#Layers: 9 #Units: 92 LR: 100 Epoch 50: train loss: 50.79051208496094\n",
            "#Layers: 9 #Units: 92 LR: 100 Training loss: 50.79051208496094 Cross validation accuracy: 0.47337278106508873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gRns_u9AdWk",
        "colab_type": "code",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "1b595b6f-f3a3-43d0-fba7-4492922c5e13"
      },
      "source": [
        "print(f'Melhor modelo: {best_model}')\n",
        "print(f'Pior modelo: {worst_model}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Melhor modelo: {'model': Feedforward(\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=2, out_features=77, bias=True)\n",
            "    (1): Linear(in_features=77, out_features=77, bias=True)\n",
            "    (2): Linear(in_features=77, out_features=1, bias=True)\n",
            "  )\n",
            "), 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), 'loss': 0.10805312544107437, 'accuracy': 0.9881656804733728}\n",
            "Pior modelo: {'model': Feedforward(\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=2, out_features=92, bias=True)\n",
            "    (1): Linear(in_features=92, out_features=92, bias=True)\n",
            "    (2): Linear(in_features=92, out_features=92, bias=True)\n",
            "    (3): Linear(in_features=92, out_features=1, bias=True)\n",
            "  )\n",
            "), 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 10\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), 'loss': 50.79051208496094, 'accuracy': 0.47337278106508873}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdJ3v1j9gFr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_decision_graph(x, model, y_true):\n",
        "  x0_contour = torch.linspace(min(x[:, 0])-0.2, max(x[:, 0])+0.2, 100)\n",
        "  x1_contour = torch.linspace(min(x[:, 1])-0.2, max(x[:, 1])+0.2, 100)\n",
        "\n",
        "  y_contour = np.zeros((len(x0_contour), len(x1_contour)))\n",
        "  for idx, x1_item in enumerate(x1_contour):\n",
        "    x0_array = x0_contour.numpy()\n",
        "    x1_array = np.full_like(x0_array, x1_item)\n",
        "    y_pred_logits = model(torch.tensor(np.stack((x0_array, x1_array), axis=-1)))\n",
        "    y_pred_labels = [1 if value >= 0.5 else 0 for value in y_pred_logits]\n",
        "    y_contour[idx] = y_pred_labels\n",
        "  \n",
        "  colorscale = [[0., 'gold'], [1., 'mediumturquoise']]\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Contour(\n",
        "        z=y_contour,\n",
        "        x=x0_contour.numpy(), # horizontal axis\n",
        "        y=x1_contour.numpy(), # vertical axis\n",
        "        opacity=0.3,\n",
        "        contours=dict(\n",
        "            start=0,\n",
        "            end=1,\n",
        "            size=1,\n",
        "        ),\n",
        "        colorscale=colorscale\n",
        "    ))\n",
        "  fig.add_trace(go.Scatter(\n",
        "        marker_color=[1. if value == True else 0. for value in (y_true.squeeze().numpy() >= 0.5)],\n",
        "        x=x[:, 0], # horizontal axis\n",
        "        y=x[:, 1], # vertical axis\n",
        "        mode='markers',\n",
        "        marker={'colorscale': [[0., 'red'], [1., 'blue']] }\n",
        "    ))\n",
        "  fig.show()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOl1V5sNg3h-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "c314265c-10a3-483f-e12e-b1d5a8e1b6b8"
      },
      "source": [
        "plot_decision_graph(x_test, best_model['model'], y_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning:\n",
            "\n",
            "nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9e63525c-16d5-4234-bb10-ada1aa0e6953\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9e63525c-16d5-4234-bb10-ada1aa0e6953\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9e63525c-16d5-4234-bb10-ada1aa0e6953',\n",
              "                        [{\"colorscale\": [[0.0, \"gold\"], [1.0, \"mediumturquoise\"]], \"contours\": {\"end\": 1, \"size\": 1, \"start\": 0}, \"opacity\": 0.3, \"type\": \"contour\", \"x\": [-1.4956735372543335, -1.455288290977478, -1.4149030447006226, -1.374517798423767, -1.334132432937622, -1.2937471866607666, -1.2533619403839111, -1.2129766941070557, -1.1725914478302002, -1.1322062015533447, -1.0918209552764893, -1.0514357089996338, -1.0110504627227783, -0.9706651568412781, -0.9302798509597778, -0.8898946046829224, -0.8495093584060669, -0.8091241121292114, -0.768738865852356, -0.7283535599708557, -0.6879683136940002, -0.6475830674171448, -0.6071977615356445, -0.5668125152587891, -0.5264272689819336, -0.4860420227050781, -0.44565674662590027, -0.4052714705467224, -0.36488622426986694, -0.3245009779930115, -0.2841157019138336, -0.24373042583465576, -0.2033451795578003, -0.16295991837978363, -0.12257465720176697, -0.0821893960237503, -0.04180413484573364, -0.00141887366771698, 0.03896638751029968, 0.07935166358947754, 0.11973690986633301, 0.16012217104434967, 0.20050743222236633, 0.240892693400383, 0.28127795457839966, 0.3216632008552551, 0.362048476934433, 0.40243375301361084, 0.4428189992904663, 0.4832042455673218, 0.5235894918441772, 0.5639747977256775, 0.604360044002533, 0.6447452902793884, 0.6851305961608887, 0.7255158424377441, 0.7659009099006653, 0.8062861561775208, 0.846671462059021, 0.8870567083358765, 0.9274419546127319, 0.9678272008895874, 1.0082124471664429, 1.048597812652588, 1.0889830589294434, 1.1293683052062988, 1.1697535514831543, 1.2101387977600098, 1.2505240440368652, 1.2909094095230103, 1.3312946557998657, 1.3716799020767212, 1.4120651483535767, 1.4524503946304321, 1.4928356409072876, 1.533220887184143, 1.573606252670288, 1.6139914989471436, 1.654376745223999, 1.6947619915008545, 1.73514723777771, 1.7755324840545654, 1.815917730331421, 1.8563029766082764, 1.8966882228851318, 1.9370735883712769, 1.9774588346481323, 2.0178442001342773, 2.0582292079925537, 2.098614454269409, 2.1389997005462646, 2.17938494682312, 2.2197701930999756, 2.260155439376831, 2.3005406856536865, 2.340926170349121, 2.3813114166259766, 2.421696662902832, 2.4620819091796875, 2.502467155456543], \"y\": [-1.1563385725021362, -1.1312869787216187, -1.1062352657318115, -1.081183671951294, -1.0561320781707764, -1.0310803651809692, -1.0060287714004517, -0.9809771180152893, -0.955925464630127, -0.9308738112449646, -0.905822217464447, -0.8807705640792847, -0.8557189106941223, -0.83066725730896, -0.8056156635284424, -0.78056401014328, -0.7555123567581177, -0.7304607033729553, -0.7054091095924377, -0.6803574562072754, -0.655305802822113, -0.6302541494369507, -0.6052025556564331, -0.5801509022712708, -0.5550993084907532, -0.5300476551055908, -0.5049960613250732, -0.4799444079399109, -0.45489275455474854, -0.42984113097190857, -0.4047895073890686, -0.37973785400390625, -0.3546862006187439, -0.32963457703590393, -0.3045829236507416, -0.2795313000679016, -0.25447964668273926, -0.2294280230998993, -0.20437638461589813, -0.17932474613189697, -0.1542731076478958, -0.12922146916389465, -0.1041698306798935, -0.07911819964647293, -0.05406656116247177, -0.02901493012905121, -0.003963291645050049, 0.02108834683895111, 0.04613998532295227, 0.07119162380695343, 0.09624326229095459, 0.12129489332437515, 0.1463465392589569, 0.17139816284179688, 0.19644980132579803, 0.2215014398097992, 0.24655300378799438, 0.27160462737083435, 0.2966562807559967, 0.32170790433883667, 0.346759557723999, 0.371811181306839, 0.39686280488967896, 0.4219144582748413, 0.44696611166000366, 0.47201773524284363, 0.497069388628006, 0.522121012210846, 0.5471726655960083, 0.5722242593765259, 0.5972759127616882, 0.6223275661468506, 0.6473792195320129, 0.6724308729171753, 0.6974824666976929, 0.7225341200828552, 0.7475857734680176, 0.7726373672485352, 0.7976890206336975, 0.8227406740188599, 0.8477922677993774, 0.8728439211845398, 0.8978955149650574, 0.9229471683502197, 0.9479988217353821, 0.9730504751205444, 0.998102068901062, 1.0231537818908691, 1.0482053756713867, 1.0732569694519043, 1.0983086824417114, 1.123360276222229, 1.1484118700027466, 1.1734635829925537, 1.1985151767730713, 1.2235667705535889, 1.248618483543396, 1.2736700773239136, 1.2987217903137207, 1.3237733840942383], \"z\": [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}, {\"marker\": {\"color\": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], \"colorscale\": [[0.0, \"red\"], [1.0, \"blue\"]]}, \"mode\": \"markers\", \"type\": \"scatter\", \"x\": [-0.2195529341697693, 1.71726655960083, 0.6111965179443359, 1.0068742036819458, 0.6615545153617859, 0.8148901462554932, 0.02452496998012066, 1.117169737815857, 0.8097688555717468, 0.23705820739269257, -0.3288789689540863, 0.9436891078948975, 1.60834801197052, 0.42076122760772705, 0.94852215051651, -0.0884881317615509, 0.7685025334358215, -0.5165951251983643, -0.24925480782985687, 0.15315845608711243, 1.018369197845459, 0.46761319041252136, 1.653892993927002, 0.8424161076545715, 1.6254273653030396, 0.07884480059146881, -0.8348286747932434, 0.028171176090836525, 0.564186692237854, -0.004683734383434057, 0.9894070625305176, 1.9721876382827759, 0.44472426176071167, 1.5508190393447876, 0.8837689757347107, -0.8626212477684021, -0.7589788436889648, -0.3218252956867218, 0.017763851210474968, 0.8341207504272461, -0.9876783490180969, 0.4481237828731537, 1.292937159538269, 1.8405194282531738, 0.49758949875831604, 1.4407426118850708, -0.07847709208726883, -0.5537376999855042, -0.452039510011673, 0.7824926376342773, 0.1422467976808548, 1.643975853919983, 0.06771350651979446, -0.8148553967475891, 1.080406665802002, 0.6952574253082275, 1.0876671075820923, 0.8210187554359436, -0.20036442577838898, -0.6171499490737915, 0.3042774796485901, 0.4202386140823364, 0.865739107131958, 1.005566954612732, 1.341210961341858, 1.760857343673706, 0.8743825554847717, 0.2235218584537506, 1.2345080375671387, -0.3246522545814514, 1.097166895866394, 0.7437918186187744, 0.001403170870617032, 0.8684070110321045, -0.03722256049513817, 0.9881172180175781, 2.0994694232940674, -0.7831920981407166, -0.6899749040603638, 0.4961864650249481, 1.5315158367156982, -1.0981365442276, 1.7253212928771973, 1.692125678062439, 0.5104709267616272, 0.41739359498023987, 0.16638633608818054, 1.8510721921920776, -0.8310459852218628, 1.3083354234695435, -0.022277235984802246, 0.12569965422153473, -0.8807494640350342, 0.6483368873596191, -0.9177405834197998, -0.10482936352491379, 2.022937059402466, 2.0525448322296143, 0.876046359539032, -0.6994197368621826, -0.09989627450704575, 0.9183590412139893, 0.8597906827926636, 1.1013681888580322, 1.158766508102417, 0.18992796540260315, 1.9610573053359985, -0.19111447036266327, 1.4594197273254395, 1.2970002889633179, -0.8763415813446045, -0.9555515646934509, 0.3988898992538452, 1.2842786312103271, 0.03876974433660507, -0.8024324178695679, 0.795286238193512, -0.583489716053009, -0.3712014853954315, 0.6531753540039062, 0.3533099889755249, -0.9564187526702881, 0.8808407783508301, -0.10623020678758621, 0.07102818787097931, -0.40381693840026855, -0.3313486874103546, 0.8137378096580505, -0.8944907188415527, 1.116707444190979, -0.012625401839613914, 0.9915586113929749, -0.15572698414325714, -0.40986061096191406, 2.0366435050964355, 0.04398626089096069, -0.3490685820579529, -0.7340160012245178, 1.6168586015701294, 1.378916621208191, 0.13204261660575867, 1.2563538551330566, 0.5977376103401184, 0.5872617363929749, 0.9566989541053772, -0.5082160830497742, 1.1549391746520996, 1.5008546113967896, 1.043332815170288, 1.2618650197982788, 1.6770411729812622, -0.4008674621582031, 0.9475045800209045, 0.9732422828674316, 0.6166980266571045, -0.010795778594911098, 1.6182905435562134, 0.8245804905891418, 0.30756044387817383, 0.22836712002754211, 1.69677734375, 0.6126381158828735, 0.16852177679538727, -0.18701380491256714, -0.7524714469909668, 2.0349557399749756, -0.7923417091369629, -0.7474494576454163, -0.0986957997083664, 0.10991643369197845, 1.1925511360168457, 1.7448596954345703, -0.09898602962493896, 0.9214462041854858, 1.8257616758346558, -0.828934907913208, 1.6482222080230713, 1.8992657661437988, 0.8245636820793152, 0.023475980386137962, 0.6758997440338135, 0.2813531756401062, -0.6105912327766418, -0.8060641288757324, 1.2452973127365112, 0.012901652604341507, 1.9775539636611938, -0.8473452925682068, 1.4752014875411987, 1.6285247802734375, 2.302467107772827, -0.21915625035762787, 0.4749681353569031, 0.29259273409843445, -0.5528075695037842, 0.18898025155067444, 0.8304718732833862, 0.5312601327896118, 0.9048495888710022, 1.8236608505249023, 0.18440605700016022, -0.8041835427284241, -0.17600971460342407, 0.542489230632782, 2.0468106269836426, 0.11032237857580185, -0.7237361669540405, 0.9945985674858093, 1.2596689462661743, 1.6678835153579712, -1.2956734895706177, 0.22904889285564423, -0.8958368301391602, 0.12359314411878586, 0.8084580898284912, -0.9793142676353455, 0.7818648219108582, 0.737747311592102, 1.2389236688613892, 0.35156217217445374, -0.05891799554228783, 0.9252936840057373, 0.9731830954551697, -0.9552695155143738, 1.6895239353179932], \"y\": [1.0097928047180176, 0.10025414824485779, -0.6239691376686096, 0.15903256833553314, 0.5422034859657288, 0.6794337034225464, 0.14413419365882874, -0.32697439193725586, 0.6172422170639038, -0.2905600070953369, 1.0639311075210571, 0.4297727942466736, -0.15478694438934326, 0.8774075508117676, 0.33060652017593384, 0.6475799083709717, -0.30957481265068054, 0.8306072950363159, 0.8598657250404358, 0.7328535914421082, -0.5696839690208435, -0.37518903613090515, -0.17637929320335388, 0.1159801334142685, -0.16288407146930695, 0.0704365074634552, 0.618267297744751, 0.5417925715446472, -0.6218916177749634, 0.22989779710769653, 0.08264066278934479, 0.3034646809101105, -0.8147494196891785, -0.5999736189842224, 0.21940003335475922, 0.31724825501441956, 0.46683743596076965, 0.9618778824806213, 0.432125449180603, -0.7041979432106018, 0.26441627740859985, 1.0645629167556763, -0.6002954840660095, 0.06981693208217621, 0.8190808892250061, -0.5870627760887146, 0.40737947821617126, 0.9951403737068176, 0.8737959861755371, -0.5113200545310974, 0.33567753434181213, -0.31155890226364136, -0.2082292139530182, 0.466535747051239, -0.3016435205936432, -0.579520583152771, -0.7057590484619141, 0.4165952503681183, 0.9508962631225586, 0.8446024656295776, 0.6923640966415405, 1.0470962524414062, -0.6114442944526672, 0.44697636365890503, -0.21514953672885895, 0.1942826807498932, 0.3522990942001343, 1.1237733364105225, -0.46158933639526367, 0.8812733292579651, -0.48151910305023193, 0.4333895444869995, 0.2331942766904831, 0.3141677975654602, -0.2134641408920288, 0.20709429681301117, 0.23272527754306793, 0.6216517090797424, 0.7775757908821106, 0.4620811641216278, -0.3815670311450958, 0.33716997504234314, -0.12623339891433716, 0.22087682783603668, -0.9563385248184204, 0.9865943193435669, 0.09117665886878967, 0.03743598610162735, 0.4495035707950592, 0.2074483036994934, 0.34779298305511475, 0.21662156283855438, 0.7794263958930969, 0.7142287492752075, -0.10658366233110428, 0.7977428436279297, 0.06994225829839706, 0.5358023643493652, 0.8437179923057556, 0.9198876023292542, 1.005160927772522, 0.4432764947414398, 0.314224511384964, -0.2599583864212036, -0.6173898577690125, 1.116766095161438, 0.25781071186065674, 0.9421854019165039, -0.21616624295711517, -0.4219587743282318, 0.2936059832572937, 0.1964997947216034, 0.883304238319397, -0.24583269655704498, -0.21961422264575958, 0.47482961416244507, 0.7884260416030884, 0.7527878284454346, 0.9245704412460327, -0.48379430174827576, 0.6652620434761047, -0.15996964275836945, 0.208968386054039, 0.6813125014305115, -0.15840822458267212, 0.8817326426506042, 0.8781833648681641, 0.10560068488121033, 0.03611943498253822, 0.39458540081977844, 0.13527344167232513, 0.39695024490356445, 1.086633324623108, 0.8914977312088013, 0.63527911901474, 0.8903142213821411, 0.9584749937057495, 0.21667072176933289, -0.206374391913414, -0.7132481932640076, -0.28387266397476196, 0.18691328167915344, 0.6797192692756653, 0.8570422530174255, 0.380867600440979, 0.866341769695282, -0.1283489465713501, -0.6677268147468567, -0.7411544919013977, -0.40745168924331665, 0.2932504415512085, 0.9132055044174194, 0.7452213764190674, 0.7794544100761414, 0.7388361692428589, 0.7409374117851257, 0.2773139178752899, 0.09402388334274292, -0.2596929967403412, 0.16561201214790344, -0.07658249884843826, -0.2956826388835907, 0.3322243094444275, 0.8029199242591858, 0.8289076685905457, 0.5168405771255493, 0.3110769987106323, 0.5075927972793579, -0.05738561227917671, -0.3505452573299408, -0.8099976778030396, 0.08024536073207855, 0.7504944205284119, 0.8603119850158691, -0.5003126859664917, 0.38824597001075745, -0.0608416348695755, 0.029741035774350166, 0.4370006024837494, 0.5348271727561951, 0.42617377638816833, 0.6492303609848022, 0.8195073008537292, 0.34001535177230835, 0.16867555677890778, 0.2746177911758423, 0.08775626122951508, 0.4363322854042053, -0.3299218714237213, -0.2093024104833603, 0.2591952383518219, 0.871330976486206, -0.36087915301322937, 0.9931857585906982, 0.7820889353752136, 0.561311662197113, 0.4710095226764679, 0.9662090539932251, 0.31276947259902954, 0.23516413569450378, 0.04460936412215233, 0.8629873991012573, 1.0653972625732422, 0.8244636654853821, 0.6959104537963867, -0.161787211894989, 0.7779713869094849, -0.40483763813972473, -0.28663113713264465, -0.25407785177230835, -0.04595986753702164, 1.0727647542953491, 0.43175894021987915, 0.6886985301971436, -0.40593084692955017, 0.15035004913806915, -0.35633206367492676, -0.7816981077194214, -0.46990156173706055, -0.42255955934524536, 0.5913589596748352, 0.022082902491092682, 0.41157984733581543, 0.5133641362190247, 0.13102102279663086]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9e63525c-16d5-4234-bb10-ada1aa0e6953');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmqWFy6qu5yJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "904e1901-522d-4266-984a-e980b8dad933"
      },
      "source": [
        "plot_decision_graph(x_test, worst_model['model'], y_test)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning:\n",
            "\n",
            "nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ea3aa83a-ebd6-41a4-820a-23a9b0d7ac28\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ea3aa83a-ebd6-41a4-820a-23a9b0d7ac28\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ea3aa83a-ebd6-41a4-820a-23a9b0d7ac28',\n",
              "                        [{\"colorscale\": [[0.0, \"gold\"], [1.0, \"mediumturquoise\"]], \"contours\": {\"end\": 1, \"size\": 1, \"start\": 0}, \"opacity\": 0.3, \"type\": \"contour\", \"x\": [-1.4956735372543335, -1.455288290977478, -1.4149030447006226, -1.374517798423767, -1.334132432937622, -1.2937471866607666, -1.2533619403839111, -1.2129766941070557, -1.1725914478302002, -1.1322062015533447, -1.0918209552764893, -1.0514357089996338, -1.0110504627227783, -0.9706651568412781, -0.9302798509597778, -0.8898946046829224, -0.8495093584060669, -0.8091241121292114, -0.768738865852356, -0.7283535599708557, -0.6879683136940002, -0.6475830674171448, -0.6071977615356445, -0.5668125152587891, -0.5264272689819336, -0.4860420227050781, -0.44565674662590027, -0.4052714705467224, -0.36488622426986694, -0.3245009779930115, -0.2841157019138336, -0.24373042583465576, -0.2033451795578003, -0.16295991837978363, -0.12257465720176697, -0.0821893960237503, -0.04180413484573364, -0.00141887366771698, 0.03896638751029968, 0.07935166358947754, 0.11973690986633301, 0.16012217104434967, 0.20050743222236633, 0.240892693400383, 0.28127795457839966, 0.3216632008552551, 0.362048476934433, 0.40243375301361084, 0.4428189992904663, 0.4832042455673218, 0.5235894918441772, 0.5639747977256775, 0.604360044002533, 0.6447452902793884, 0.6851305961608887, 0.7255158424377441, 0.7659009099006653, 0.8062861561775208, 0.846671462059021, 0.8870567083358765, 0.9274419546127319, 0.9678272008895874, 1.0082124471664429, 1.048597812652588, 1.0889830589294434, 1.1293683052062988, 1.1697535514831543, 1.2101387977600098, 1.2505240440368652, 1.2909094095230103, 1.3312946557998657, 1.3716799020767212, 1.4120651483535767, 1.4524503946304321, 1.4928356409072876, 1.533220887184143, 1.573606252670288, 1.6139914989471436, 1.654376745223999, 1.6947619915008545, 1.73514723777771, 1.7755324840545654, 1.815917730331421, 1.8563029766082764, 1.8966882228851318, 1.9370735883712769, 1.9774588346481323, 2.0178442001342773, 2.0582292079925537, 2.098614454269409, 2.1389997005462646, 2.17938494682312, 2.2197701930999756, 2.260155439376831, 2.3005406856536865, 2.340926170349121, 2.3813114166259766, 2.421696662902832, 2.4620819091796875, 2.502467155456543], \"y\": [-1.1563385725021362, -1.1312869787216187, -1.1062352657318115, -1.081183671951294, -1.0561320781707764, -1.0310803651809692, -1.0060287714004517, -0.9809771180152893, -0.955925464630127, -0.9308738112449646, -0.905822217464447, -0.8807705640792847, -0.8557189106941223, -0.83066725730896, -0.8056156635284424, -0.78056401014328, -0.7555123567581177, -0.7304607033729553, -0.7054091095924377, -0.6803574562072754, -0.655305802822113, -0.6302541494369507, -0.6052025556564331, -0.5801509022712708, -0.5550993084907532, -0.5300476551055908, -0.5049960613250732, -0.4799444079399109, -0.45489275455474854, -0.42984113097190857, -0.4047895073890686, -0.37973785400390625, -0.3546862006187439, -0.32963457703590393, -0.3045829236507416, -0.2795313000679016, -0.25447964668273926, -0.2294280230998993, -0.20437638461589813, -0.17932474613189697, -0.1542731076478958, -0.12922146916389465, -0.1041698306798935, -0.07911819964647293, -0.05406656116247177, -0.02901493012905121, -0.003963291645050049, 0.02108834683895111, 0.04613998532295227, 0.07119162380695343, 0.09624326229095459, 0.12129489332437515, 0.1463465392589569, 0.17139816284179688, 0.19644980132579803, 0.2215014398097992, 0.24655300378799438, 0.27160462737083435, 0.2966562807559967, 0.32170790433883667, 0.346759557723999, 0.371811181306839, 0.39686280488967896, 0.4219144582748413, 0.44696611166000366, 0.47201773524284363, 0.497069388628006, 0.522121012210846, 0.5471726655960083, 0.5722242593765259, 0.5972759127616882, 0.6223275661468506, 0.6473792195320129, 0.6724308729171753, 0.6974824666976929, 0.7225341200828552, 0.7475857734680176, 0.7726373672485352, 0.7976890206336975, 0.8227406740188599, 0.8477922677993774, 0.8728439211845398, 0.8978955149650574, 0.9229471683502197, 0.9479988217353821, 0.9730504751205444, 0.998102068901062, 1.0231537818908691, 1.0482053756713867, 1.0732569694519043, 1.0983086824417114, 1.123360276222229, 1.1484118700027466, 1.1734635829925537, 1.1985151767730713, 1.2235667705535889, 1.248618483543396, 1.2736700773239136, 1.2987217903137207, 1.3237733840942383], \"z\": [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}, {\"marker\": {\"color\": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], \"colorscale\": [[0.0, \"red\"], [1.0, \"blue\"]]}, \"mode\": \"markers\", \"type\": \"scatter\", \"x\": [-0.2195529341697693, 1.71726655960083, 0.6111965179443359, 1.0068742036819458, 0.6615545153617859, 0.8148901462554932, 0.02452496998012066, 1.117169737815857, 0.8097688555717468, 0.23705820739269257, -0.3288789689540863, 0.9436891078948975, 1.60834801197052, 0.42076122760772705, 0.94852215051651, -0.0884881317615509, 0.7685025334358215, -0.5165951251983643, -0.24925480782985687, 0.15315845608711243, 1.018369197845459, 0.46761319041252136, 1.653892993927002, 0.8424161076545715, 1.6254273653030396, 0.07884480059146881, -0.8348286747932434, 0.028171176090836525, 0.564186692237854, -0.004683734383434057, 0.9894070625305176, 1.9721876382827759, 0.44472426176071167, 1.5508190393447876, 0.8837689757347107, -0.8626212477684021, -0.7589788436889648, -0.3218252956867218, 0.017763851210474968, 0.8341207504272461, -0.9876783490180969, 0.4481237828731537, 1.292937159538269, 1.8405194282531738, 0.49758949875831604, 1.4407426118850708, -0.07847709208726883, -0.5537376999855042, -0.452039510011673, 0.7824926376342773, 0.1422467976808548, 1.643975853919983, 0.06771350651979446, -0.8148553967475891, 1.080406665802002, 0.6952574253082275, 1.0876671075820923, 0.8210187554359436, -0.20036442577838898, -0.6171499490737915, 0.3042774796485901, 0.4202386140823364, 0.865739107131958, 1.005566954612732, 1.341210961341858, 1.760857343673706, 0.8743825554847717, 0.2235218584537506, 1.2345080375671387, -0.3246522545814514, 1.097166895866394, 0.7437918186187744, 0.001403170870617032, 0.8684070110321045, -0.03722256049513817, 0.9881172180175781, 2.0994694232940674, -0.7831920981407166, -0.6899749040603638, 0.4961864650249481, 1.5315158367156982, -1.0981365442276, 1.7253212928771973, 1.692125678062439, 0.5104709267616272, 0.41739359498023987, 0.16638633608818054, 1.8510721921920776, -0.8310459852218628, 1.3083354234695435, -0.022277235984802246, 0.12569965422153473, -0.8807494640350342, 0.6483368873596191, -0.9177405834197998, -0.10482936352491379, 2.022937059402466, 2.0525448322296143, 0.876046359539032, -0.6994197368621826, -0.09989627450704575, 0.9183590412139893, 0.8597906827926636, 1.1013681888580322, 1.158766508102417, 0.18992796540260315, 1.9610573053359985, -0.19111447036266327, 1.4594197273254395, 1.2970002889633179, -0.8763415813446045, -0.9555515646934509, 0.3988898992538452, 1.2842786312103271, 0.03876974433660507, -0.8024324178695679, 0.795286238193512, -0.583489716053009, -0.3712014853954315, 0.6531753540039062, 0.3533099889755249, -0.9564187526702881, 0.8808407783508301, -0.10623020678758621, 0.07102818787097931, -0.40381693840026855, -0.3313486874103546, 0.8137378096580505, -0.8944907188415527, 1.116707444190979, -0.012625401839613914, 0.9915586113929749, -0.15572698414325714, -0.40986061096191406, 2.0366435050964355, 0.04398626089096069, -0.3490685820579529, -0.7340160012245178, 1.6168586015701294, 1.378916621208191, 0.13204261660575867, 1.2563538551330566, 0.5977376103401184, 0.5872617363929749, 0.9566989541053772, -0.5082160830497742, 1.1549391746520996, 1.5008546113967896, 1.043332815170288, 1.2618650197982788, 1.6770411729812622, -0.4008674621582031, 0.9475045800209045, 0.9732422828674316, 0.6166980266571045, -0.010795778594911098, 1.6182905435562134, 0.8245804905891418, 0.30756044387817383, 0.22836712002754211, 1.69677734375, 0.6126381158828735, 0.16852177679538727, -0.18701380491256714, -0.7524714469909668, 2.0349557399749756, -0.7923417091369629, -0.7474494576454163, -0.0986957997083664, 0.10991643369197845, 1.1925511360168457, 1.7448596954345703, -0.09898602962493896, 0.9214462041854858, 1.8257616758346558, -0.828934907913208, 1.6482222080230713, 1.8992657661437988, 0.8245636820793152, 0.023475980386137962, 0.6758997440338135, 0.2813531756401062, -0.6105912327766418, -0.8060641288757324, 1.2452973127365112, 0.012901652604341507, 1.9775539636611938, -0.8473452925682068, 1.4752014875411987, 1.6285247802734375, 2.302467107772827, -0.21915625035762787, 0.4749681353569031, 0.29259273409843445, -0.5528075695037842, 0.18898025155067444, 0.8304718732833862, 0.5312601327896118, 0.9048495888710022, 1.8236608505249023, 0.18440605700016022, -0.8041835427284241, -0.17600971460342407, 0.542489230632782, 2.0468106269836426, 0.11032237857580185, -0.7237361669540405, 0.9945985674858093, 1.2596689462661743, 1.6678835153579712, -1.2956734895706177, 0.22904889285564423, -0.8958368301391602, 0.12359314411878586, 0.8084580898284912, -0.9793142676353455, 0.7818648219108582, 0.737747311592102, 1.2389236688613892, 0.35156217217445374, -0.05891799554228783, 0.9252936840057373, 0.9731830954551697, -0.9552695155143738, 1.6895239353179932], \"y\": [1.0097928047180176, 0.10025414824485779, -0.6239691376686096, 0.15903256833553314, 0.5422034859657288, 0.6794337034225464, 0.14413419365882874, -0.32697439193725586, 0.6172422170639038, -0.2905600070953369, 1.0639311075210571, 0.4297727942466736, -0.15478694438934326, 0.8774075508117676, 0.33060652017593384, 0.6475799083709717, -0.30957481265068054, 0.8306072950363159, 0.8598657250404358, 0.7328535914421082, -0.5696839690208435, -0.37518903613090515, -0.17637929320335388, 0.1159801334142685, -0.16288407146930695, 0.0704365074634552, 0.618267297744751, 0.5417925715446472, -0.6218916177749634, 0.22989779710769653, 0.08264066278934479, 0.3034646809101105, -0.8147494196891785, -0.5999736189842224, 0.21940003335475922, 0.31724825501441956, 0.46683743596076965, 0.9618778824806213, 0.432125449180603, -0.7041979432106018, 0.26441627740859985, 1.0645629167556763, -0.6002954840660095, 0.06981693208217621, 0.8190808892250061, -0.5870627760887146, 0.40737947821617126, 0.9951403737068176, 0.8737959861755371, -0.5113200545310974, 0.33567753434181213, -0.31155890226364136, -0.2082292139530182, 0.466535747051239, -0.3016435205936432, -0.579520583152771, -0.7057590484619141, 0.4165952503681183, 0.9508962631225586, 0.8446024656295776, 0.6923640966415405, 1.0470962524414062, -0.6114442944526672, 0.44697636365890503, -0.21514953672885895, 0.1942826807498932, 0.3522990942001343, 1.1237733364105225, -0.46158933639526367, 0.8812733292579651, -0.48151910305023193, 0.4333895444869995, 0.2331942766904831, 0.3141677975654602, -0.2134641408920288, 0.20709429681301117, 0.23272527754306793, 0.6216517090797424, 0.7775757908821106, 0.4620811641216278, -0.3815670311450958, 0.33716997504234314, -0.12623339891433716, 0.22087682783603668, -0.9563385248184204, 0.9865943193435669, 0.09117665886878967, 0.03743598610162735, 0.4495035707950592, 0.2074483036994934, 0.34779298305511475, 0.21662156283855438, 0.7794263958930969, 0.7142287492752075, -0.10658366233110428, 0.7977428436279297, 0.06994225829839706, 0.5358023643493652, 0.8437179923057556, 0.9198876023292542, 1.005160927772522, 0.4432764947414398, 0.314224511384964, -0.2599583864212036, -0.6173898577690125, 1.116766095161438, 0.25781071186065674, 0.9421854019165039, -0.21616624295711517, -0.4219587743282318, 0.2936059832572937, 0.1964997947216034, 0.883304238319397, -0.24583269655704498, -0.21961422264575958, 0.47482961416244507, 0.7884260416030884, 0.7527878284454346, 0.9245704412460327, -0.48379430174827576, 0.6652620434761047, -0.15996964275836945, 0.208968386054039, 0.6813125014305115, -0.15840822458267212, 0.8817326426506042, 0.8781833648681641, 0.10560068488121033, 0.03611943498253822, 0.39458540081977844, 0.13527344167232513, 0.39695024490356445, 1.086633324623108, 0.8914977312088013, 0.63527911901474, 0.8903142213821411, 0.9584749937057495, 0.21667072176933289, -0.206374391913414, -0.7132481932640076, -0.28387266397476196, 0.18691328167915344, 0.6797192692756653, 0.8570422530174255, 0.380867600440979, 0.866341769695282, -0.1283489465713501, -0.6677268147468567, -0.7411544919013977, -0.40745168924331665, 0.2932504415512085, 0.9132055044174194, 0.7452213764190674, 0.7794544100761414, 0.7388361692428589, 0.7409374117851257, 0.2773139178752899, 0.09402388334274292, -0.2596929967403412, 0.16561201214790344, -0.07658249884843826, -0.2956826388835907, 0.3322243094444275, 0.8029199242591858, 0.8289076685905457, 0.5168405771255493, 0.3110769987106323, 0.5075927972793579, -0.05738561227917671, -0.3505452573299408, -0.8099976778030396, 0.08024536073207855, 0.7504944205284119, 0.8603119850158691, -0.5003126859664917, 0.38824597001075745, -0.0608416348695755, 0.029741035774350166, 0.4370006024837494, 0.5348271727561951, 0.42617377638816833, 0.6492303609848022, 0.8195073008537292, 0.34001535177230835, 0.16867555677890778, 0.2746177911758423, 0.08775626122951508, 0.4363322854042053, -0.3299218714237213, -0.2093024104833603, 0.2591952383518219, 0.871330976486206, -0.36087915301322937, 0.9931857585906982, 0.7820889353752136, 0.561311662197113, 0.4710095226764679, 0.9662090539932251, 0.31276947259902954, 0.23516413569450378, 0.04460936412215233, 0.8629873991012573, 1.0653972625732422, 0.8244636654853821, 0.6959104537963867, -0.161787211894989, 0.7779713869094849, -0.40483763813972473, -0.28663113713264465, -0.25407785177230835, -0.04595986753702164, 1.0727647542953491, 0.43175894021987915, 0.6886985301971436, -0.40593084692955017, 0.15035004913806915, -0.35633206367492676, -0.7816981077194214, -0.46990156173706055, -0.42255955934524536, 0.5913589596748352, 0.022082902491092682, 0.41157984733581543, 0.5133641362190247, 0.13102102279663086]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ea3aa83a-ebd6-41a4-820a-23a9b0d7ac28');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfKYMSWKwgNN",
        "colab_type": "text"
      },
      "source": [
        "**Questão 2.** Neste exercício, você deve escolher um dataset de sua preferência (exceto Boston Housing e MNIST) para avaliar redes neurais. Você deve minimamente:\n",
        "* Explicar o problema associado ao dataset escolhido;\n",
        "* Comparar a sua rede neural com um modelo mais simples (ex: k-NN ou regress ̃ao logística);\n",
        "* Plotar a evolução da função custo (loss) ao longo do treinamento (épocas) para verificar a corretude do algoritmo de treinamento.\n",
        "* Reportar taxas de acerto (ou erro) nos conjuntos de treinamento e teste para diferentes configurações da rede neural.\n",
        "\n",
        "Obs: UCI Datasets (https://archive.ics.uci.edu/ml/datasets.php) e a biblioteca TorchVision (https://pytorch.org/docs/stable/torchvision/datasets.html) possuem várias opções de datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh0fJlUnf9vr",
        "colab_type": "text"
      },
      "source": [
        "O dataset escolhido foi o Kaggle Dogs vs Cats, disponível em https://www.microsoft.com/en-us/download/details.aspx?id=54765 e está contido na bilbioteca de datasets do Tensorflow, disponível em https://www.tensorflow.org/datasets/catalog/cats_vs_dogs. Como o nome sugere, o dataset visa classificar imagens de gatos ou cachorros, podendo ser um problema de classificação binária. Aqui as imagens serão centralizadas em um tamanho razoável para a identificação e não serão feitas análises de alternativas de redes que possam resolver bem esse problema, visto que o intuito principal é demonstrar o comportamento de uma MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU0VQMNkf8ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4einECWzR6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "607cc8c2-de6b-4f5a-e1a4-e47ad176f963"
      },
      "source": [
        "!wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.193.24.126, 2600:1417:76:586::e59, 2600:1417:76:58e::e59\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.193.24.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824894548 (787M) [application/octet-stream]\n",
            "Saving to: ‘kagglecatsanddogs_3367a.zip.1’\n",
            "\n",
            "kagglecatsanddogs_3 100%[===================>] 786.68M   162MB/s    in 4.8s    \n",
            "\n",
            "(164 MB/s) - ‘kagglecatsanddogs_3367a.zip.1’ saved [824894548/824894548]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5MS0nT9zyIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "850d30ee-6437-4246-adf3-e19bb1b87325"
      },
      "source": [
        "!unzip kagglecatsanddogs_3367a.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  kagglecatsanddogs_3367a.zip\n",
            "replace PetImages/Cat/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gax57NDY0EG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5e8bb940-a92b-4cee-a005-fc58be0194e5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " kagglecatsanddogs_3367a.zip\t'MSR-LA - 3467.docx'  'readme[1].txt'\n",
            " kagglecatsanddogs_3367a.zip.1\t PetImages\t       sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV_OENTC1V7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oeUsrNz10db",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c197ff39-b9b2-4824-db54-7f901a3a3303"
      },
      "source": [
        "device"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95cP722RtGwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "02492382-b1ed-4b30-925d-7a1549e535a7"
      },
      "source": [
        "folder = 'PetImages'\n",
        "image_size = 128\n",
        "\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((image_size, image_size)),\n",
        "    torchvision.transforms.ToTensor(), # (3, 128, 128)\n",
        "    #torchvision.transforms.Lambda(lambda x: x.to(device)),\n",
        "    torchvision.transforms.Lambda(lambda x: x.flatten())\n",
        "])\n",
        "\n",
        "def verify_image(image):\n",
        "    try:\n",
        "        Image.open(image).verify()\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "ds = torchvision.datasets.ImageFolder(folder, transform=transforms, is_valid_file=verify_image)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning:\n",
            "\n",
            "Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning:\n",
            "\n",
            "Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_o2xUXwhzVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset_size = int(len(ds) * 0.1) # O dataset é muito grande, pegando apenas 10% dele\n",
        "ignored_size = len(ds) - subset_size\n",
        "train_size = int(0.8 * subset_size)\n",
        "test_size = subset_size - train_size\n",
        "\n",
        "subset, _ = torch.utils.data.random_split(ds, [subset_size, ignored_size])\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(subset, [train_size, test_size])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5x8cm6h2tT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5pzmKCz0UEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = torch.nn.Linear(3 * 128 * 128, 1)\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.linear(x))\n",
        "    return x\n",
        "baseline_model = LogisticRegression()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVKtTSGDA1SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2f3ac123-5a04-4a66-b8ef-faae5fca2407"
      },
      "source": [
        "best_baseline_model = {'model': None, 'optimizer': None, 'loss': sys.maxsize, 'accuracy': 0}\n",
        "learning_rates = [10**-x for x in range(4, 1, -1)]\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  baseline_model.train()\n",
        "  criterion = torch.nn.BCELoss(size_average=True)\n",
        "  optimizer = torch.optim.SGD(baseline_model.parameters(), lr=learning_rate)\n",
        "  for epoch in range(20):\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "      baseline_model.train()\n",
        "      optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "      y_pred = baseline_model(images)\n",
        "      # Compute Loss\n",
        "      loss = criterion(y_pred.squeeze(), labels.float())\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      y_pred = baseline_model(images)\n",
        "      correct += ((y_pred.squeeze() >= 0.5) == labels.float()).sum().item()\n",
        "      total += labels.shape[0]\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'LR: {learning_rate} Train loss: {loss.item()} Test accuracy: {accuracy}')\n",
        "\n",
        "    if loss.item() < best_baseline_model['loss']:\n",
        "      best_baseline_model['model'] = baseline_model\n",
        "      best_baseline_model['optimizer'] = optimizer\n",
        "      best_baseline_model['loss'] = loss.item()\n",
        "      best_baseline_model['accuracy'] = accuracy"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning:\n",
            "\n",
            "size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning:\n",
            "\n",
            "nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR: 0.0001 Train loss: 0.6929877996444702 Test accuracy: 0.508\n",
            "LR: 0.001 Train loss: 1.1297045946121216 Test accuracy: 0.482\n",
            "LR: 0.01 Train loss: 12.711209297180176 Test accuracy: 0.496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nppaYbg0QOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "  def __init__(self, input_size, output_size, number_layers, number_units):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.number_layers = number_layers\n",
        "    self.number_units = number_units\n",
        "    fc = torch.nn.ModuleList() # fully connected\n",
        "    for i in range(number_layers):\n",
        "      if i == 0:\n",
        "        fc.append(torch.nn.Linear(input_size, number_units))\n",
        "      elif i == number_layers-1:\n",
        "        fc.append(torch.nn.Linear(number_units, output_size))\n",
        "      else:\n",
        "        fc.append(torch.nn.Linear(number_units, number_units))\n",
        "    self.fc = fc\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i in range(self.number_layers):\n",
        "      if i == self.number_layers-1:\n",
        "        x = F.sigmoid(self.fc[i](x))\n",
        "      else:\n",
        "        x = F.relu(self.fc[i](x))\n",
        "    return x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0HG-WF00R36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f5b7d7c-d0ed-4f04-daf2-0af3f6457abb"
      },
      "source": [
        "number_units = range(2, 23, 10)\n",
        "number_layers = range(2, 11, 4)\n",
        "learning_rates = [10**-x for x in range(3, -1, -1)]\n",
        "epoch = 10\n",
        "\n",
        "best_model = {'model': None, 'optimizer': None, 'loss': sys.maxsize, 'accuracy': 0}\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  for number_layer in number_layers:\n",
        "    for number_unit in number_units:\n",
        "        model = FeedForward(3 * 128 * 128, 1, number_layer, number_unit)\n",
        "        criterion = torch.nn.BCELoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "        model.train()\n",
        "        for epoch in range(1, epoch+1):\n",
        "          for idx, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            y_pred = model(images)\n",
        "            # Compute Loss\n",
        "            loss = criterion(y_pred.squeeze(), labels.float())\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          print(f'#Layers: {number_layer} #Units: {number_unit} LR: {learning_rate} Epoch {epoch}: train loss: {loss.item()}')\n",
        "\n",
        "        with torch.no_grad():\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for images, labels in test_loader:\n",
        "            y_pred = model(images)\n",
        "            correct += ((y_pred.squeeze() >= 0.5) == labels.float()).sum().item()\n",
        "            total += labels.shape[0]\n",
        "\n",
        "          accuracy = correct / total\n",
        "          print(f'#Layers: {number_layer} #Units: {number_unit} LR: {learning_rate} Train loss: {loss.item()} Test accuracy: {accuracy}')\n",
        "\n",
        "          if loss.item() < best_model['loss']:\n",
        "            best_model['model'] = model\n",
        "            best_model['optimizer'] = optimizer\n",
        "            best_model['loss'] = loss.item()\n",
        "            best_model['accuracy'] = accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning:\n",
            "\n",
            "nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 1: train loss: 0.708114504814148\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 2: train loss: 0.7080109715461731\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 3: train loss: 0.7079711556434631\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 4: train loss: 0.7079395055770874\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 5: train loss: 0.7078931331634521\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 6: train loss: 0.707851767539978\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 7: train loss: 0.7078334093093872\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 8: train loss: 0.7077964544296265\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 9: train loss: 0.7077520489692688\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Epoch 10: train loss: 0.7076995372772217\n",
            "#Layers: 2 #Units: 2 LR: 0.001 Train loss: 0.7076995372772217 Test accuracy: 0.518\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 1: train loss: 0.6939381957054138\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 2: train loss: 0.692548930644989\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 3: train loss: 0.6916207671165466\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 4: train loss: 0.6907963752746582\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 5: train loss: 0.6899575591087341\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 6: train loss: 0.6891216039657593\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 7: train loss: 0.688446581363678\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 8: train loss: 0.687679648399353\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 9: train loss: 0.6869319677352905\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Epoch 10: train loss: 0.6863265037536621\n",
            "#Layers: 2 #Units: 12 LR: 0.001 Train loss: 0.6863265037536621 Test accuracy: 0.556\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 1: train loss: 0.6977216601371765\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 2: train loss: 0.6941582560539246\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 3: train loss: 0.6930862069129944\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 4: train loss: 0.6921132206916809\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 5: train loss: 0.6912973523139954\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 6: train loss: 0.6906514763832092\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 7: train loss: 0.6899852752685547\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 8: train loss: 0.6893870830535889\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 9: train loss: 0.688740074634552\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Epoch 10: train loss: 0.6881189346313477\n",
            "#Layers: 2 #Units: 22 LR: 0.001 Train loss: 0.6881189346313477 Test accuracy: 0.568\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 1: train loss: 0.6903935074806213\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 2: train loss: 0.6903741359710693\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 3: train loss: 0.6903560161590576\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 4: train loss: 0.6903368234634399\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 5: train loss: 0.6903182864189148\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 6: train loss: 0.6903006434440613\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 7: train loss: 0.6902819871902466\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 8: train loss: 0.6902637481689453\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 9: train loss: 0.6902461051940918\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Epoch 10: train loss: 0.6902281045913696\n",
            "#Layers: 6 #Units: 2 LR: 0.001 Train loss: 0.6902281045913696 Test accuracy: 0.482\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 1: train loss: 0.7089096307754517\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 2: train loss: 0.7088709473609924\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 3: train loss: 0.7088328003883362\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 4: train loss: 0.7087946534156799\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 5: train loss: 0.7087575197219849\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 6: train loss: 0.7087208032608032\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 7: train loss: 0.7086842060089111\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 8: train loss: 0.7086479067802429\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 9: train loss: 0.7086118459701538\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Epoch 10: train loss: 0.7085758447647095\n",
            "#Layers: 6 #Units: 12 LR: 0.001 Train loss: 0.7085758447647095 Test accuracy: 0.518\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 1: train loss: 0.6953304409980774\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 2: train loss: 0.6953064203262329\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 3: train loss: 0.6952826976776123\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 4: train loss: 0.6952610611915588\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 5: train loss: 0.6952430009841919\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 6: train loss: 0.6952252984046936\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 7: train loss: 0.6952073574066162\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 8: train loss: 0.6951883435249329\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 9: train loss: 0.6951709389686584\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Epoch 10: train loss: 0.6951546669006348\n",
            "#Layers: 6 #Units: 22 LR: 0.001 Train loss: 0.6951546669006348 Test accuracy: 0.518\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 1: train loss: 0.7427297830581665\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 2: train loss: 0.7426350712776184\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 3: train loss: 0.7425408363342285\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 4: train loss: 0.7424471974372864\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 5: train loss: 0.742353081703186\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 6: train loss: 0.7422598600387573\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 7: train loss: 0.7421664595603943\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 8: train loss: 0.7420734167098999\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 9: train loss: 0.7419800162315369\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Epoch 10: train loss: 0.7418871521949768\n",
            "#Layers: 10 #Units: 2 LR: 0.001 Train loss: 0.7418871521949768 Test accuracy: 0.518\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 1: train loss: 0.6886781454086304\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 2: train loss: 0.6886740922927856\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 3: train loss: 0.6886702179908752\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 4: train loss: 0.6886664628982544\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 5: train loss: 0.6886625289916992\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 6: train loss: 0.688658595085144\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 7: train loss: 0.6886549592018127\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 8: train loss: 0.6886510848999023\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 9: train loss: 0.6886473894119263\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Epoch 10: train loss: 0.6886439323425293\n",
            "#Layers: 10 #Units: 12 LR: 0.001 Train loss: 0.6886439323425293 Test accuracy: 0.482\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 1: train loss: 0.6930068135261536\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 2: train loss: 0.6930062770843506\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 3: train loss: 0.6930058598518372\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 4: train loss: 0.6930054426193237\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 5: train loss: 0.693004846572876\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 6: train loss: 0.6930044293403625\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 7: train loss: 0.6930041313171387\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 8: train loss: 0.6930036544799805\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 9: train loss: 0.6930031180381775\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Epoch 10: train loss: 0.6930027008056641\n",
            "#Layers: 10 #Units: 22 LR: 0.001 Train loss: 0.6930027008056641 Test accuracy: 0.482\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 1: train loss: 0.6933059692382812\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 2: train loss: 0.6932849884033203\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 3: train loss: 0.6932704448699951\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 4: train loss: 0.6932596564292908\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 5: train loss: 0.6932510137557983\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 6: train loss: 0.6932457685470581\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 7: train loss: 0.6932393908500671\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 8: train loss: 0.6932334899902344\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 9: train loss: 0.6932278275489807\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Epoch 10: train loss: 0.6932219862937927\n",
            "#Layers: 2 #Units: 2 LR: 0.01 Train loss: 0.6932219862937927 Test accuracy: 0.518\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 1: train loss: 0.7122859954833984\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 2: train loss: 0.7108047008514404\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 3: train loss: 0.7103683352470398\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 4: train loss: 0.7098773717880249\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 5: train loss: 0.709200382232666\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 6: train loss: 0.705324649810791\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 7: train loss: 0.7007928490638733\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 8: train loss: 0.6964794993400574\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 9: train loss: 0.6955416798591614\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Epoch 10: train loss: 0.6940237879753113\n",
            "#Layers: 2 #Units: 12 LR: 0.01 Train loss: 0.6940237879753113 Test accuracy: 0.544\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 1: train loss: 0.6870101094245911\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 2: train loss: 0.6888809204101562\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 3: train loss: 0.6868177652359009\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 4: train loss: 0.6851649284362793\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 5: train loss: 0.6822710037231445\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 6: train loss: 0.6826769113540649\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 7: train loss: 0.6807704567909241\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 8: train loss: 0.6791682243347168\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 9: train loss: 0.6778048276901245\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Epoch 10: train loss: 0.6774334907531738\n",
            "#Layers: 2 #Units: 22 LR: 0.01 Train loss: 0.6774334907531738 Test accuracy: 0.576\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 1: train loss: 0.7944291234016418\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 2: train loss: 0.7919648885726929\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 3: train loss: 0.7895302176475525\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 4: train loss: 0.7871878147125244\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 5: train loss: 0.7850314974784851\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 6: train loss: 0.783024251461029\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 7: train loss: 0.7811121344566345\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 8: train loss: 0.7792739272117615\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 9: train loss: 0.7775054574012756\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Epoch 10: train loss: 0.7757934331893921\n",
            "#Layers: 6 #Units: 2 LR: 0.01 Train loss: 0.7757934331893921 Test accuracy: 0.518\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 1: train loss: 0.7255719900131226\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 2: train loss: 0.7242274284362793\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 3: train loss: 0.7230660915374756\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 4: train loss: 0.7219279408454895\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 5: train loss: 0.7210096120834351\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 6: train loss: 0.7202578186988831\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 7: train loss: 0.7195712327957153\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 8: train loss: 0.7189164161682129\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 9: train loss: 0.7182865738868713\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Epoch 10: train loss: 0.7176830768585205\n",
            "#Layers: 6 #Units: 12 LR: 0.01 Train loss: 0.7176830768585205 Test accuracy: 0.518\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 1: train loss: 0.7029039859771729\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 2: train loss: 0.7026636004447937\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 3: train loss: 0.7024320363998413\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 4: train loss: 0.7022089958190918\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 5: train loss: 0.701992392539978\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 6: train loss: 0.7017852067947388\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 7: train loss: 0.7015821933746338\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 8: train loss: 0.7013879418373108\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 9: train loss: 0.7012030482292175\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Epoch 10: train loss: 0.7010259032249451\n",
            "#Layers: 6 #Units: 22 LR: 0.01 Train loss: 0.7010259032249451 Test accuracy: 0.518\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 1: train loss: 0.7493604421615601\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 2: train loss: 0.7484963536262512\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 3: train loss: 0.747645914554596\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 4: train loss: 0.746810257434845\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 5: train loss: 0.7459877133369446\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 6: train loss: 0.7451781630516052\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 7: train loss: 0.744381844997406\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 8: train loss: 0.7435982823371887\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 9: train loss: 0.742827296257019\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Epoch 10: train loss: 0.7420687675476074\n",
            "#Layers: 10 #Units: 2 LR: 0.01 Train loss: 0.7420687675476074 Test accuracy: 0.518\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 1: train loss: 0.689024806022644\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 2: train loss: 0.6890507340431213\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 3: train loss: 0.6890767812728882\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 4: train loss: 0.6891031265258789\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 5: train loss: 0.6891294717788696\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 6: train loss: 0.689156174659729\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 7: train loss: 0.6891827583312988\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 8: train loss: 0.6892094612121582\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 9: train loss: 0.6892366409301758\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Epoch 10: train loss: 0.6892635822296143\n",
            "#Layers: 10 #Units: 12 LR: 0.01 Train loss: 0.6892635822296143 Test accuracy: 0.482\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 1: train loss: 0.6941706538200378\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 2: train loss: 0.6941510438919067\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 3: train loss: 0.6941317319869995\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 4: train loss: 0.6941124796867371\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 5: train loss: 0.6940937042236328\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 6: train loss: 0.6940749883651733\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 7: train loss: 0.6940568089485168\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 8: train loss: 0.6940388679504395\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 9: train loss: 0.6940208077430725\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Epoch 10: train loss: 0.6940037608146667\n",
            "#Layers: 10 #Units: 22 LR: 0.01 Train loss: 0.6940037608146667 Test accuracy: 0.518\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 1: train loss: 0.6885792016983032\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 2: train loss: 0.6887359023094177\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 3: train loss: 0.6889233589172363\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 4: train loss: 0.6891293525695801\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 5: train loss: 0.6893458962440491\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 6: train loss: 0.6895661950111389\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 7: train loss: 0.689785897731781\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 8: train loss: 0.6900004744529724\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 9: train loss: 0.6902081966400146\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Epoch 10: train loss: 0.6904067993164062\n",
            "#Layers: 2 #Units: 2 LR: 0.1 Train loss: 0.6904067993164062 Test accuracy: 0.482\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 1: train loss: 0.6983723640441895\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 2: train loss: 0.697335422039032\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 3: train loss: 0.6963038444519043\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 4: train loss: 0.6950652003288269\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 5: train loss: 0.6941339373588562\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 6: train loss: 0.693462073802948\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 7: train loss: 0.692825198173523\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 8: train loss: 0.6920352578163147\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 9: train loss: 0.6914734244346619\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Epoch 10: train loss: 0.6909857988357544\n",
            "#Layers: 2 #Units: 12 LR: 0.1 Train loss: 0.6909857988357544 Test accuracy: 0.494\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 1: train loss: 5.791099548339844\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 2: train loss: 0.6932141780853271\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 3: train loss: 0.6931707262992859\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 4: train loss: 0.693130612373352\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 5: train loss: 0.6930923461914062\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 6: train loss: 0.6930407285690308\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 7: train loss: 0.692893385887146\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 8: train loss: 0.6928070783615112\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 9: train loss: 0.6927348971366882\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Epoch 10: train loss: 0.6925734877586365\n",
            "#Layers: 2 #Units: 22 LR: 0.1 Train loss: 0.6925734877586365 Test accuracy: 0.482\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 1: train loss: 0.7028722763061523\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 2: train loss: 0.7013097405433655\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 3: train loss: 0.7000113129615784\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 4: train loss: 0.6989290118217468\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 5: train loss: 0.6980238556861877\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 6: train loss: 0.6972633600234985\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 7: train loss: 0.6966227293014526\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 8: train loss: 0.696081280708313\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 9: train loss: 0.695622444152832\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Epoch 10: train loss: 0.6952319741249084\n",
            "#Layers: 6 #Units: 2 LR: 0.1 Train loss: 0.6952319741249084 Test accuracy: 0.518\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 1: train loss: 0.6903725266456604\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 2: train loss: 0.6906098127365112\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 3: train loss: 0.6908280849456787\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 4: train loss: 0.6910267472267151\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 5: train loss: 0.6912110447883606\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 6: train loss: 0.691373884677887\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 7: train loss: 0.6915148496627808\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 8: train loss: 0.6916372776031494\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 9: train loss: 0.6917358636856079\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Epoch 10: train loss: 0.6918190121650696\n",
            "#Layers: 6 #Units: 12 LR: 0.1 Train loss: 0.6918190121650696 Test accuracy: 0.482\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 1: train loss: 0.7072676420211792\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 2: train loss: 0.7028422951698303\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 3: train loss: 0.7004639506340027\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 4: train loss: 0.6988113522529602\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 5: train loss: 0.6976505517959595\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 6: train loss: 0.6967605352401733\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 7: train loss: 0.6960588693618774\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 8: train loss: 0.6954870223999023\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 9: train loss: 0.6950209736824036\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Epoch 10: train loss: 0.6946455836296082\n",
            "#Layers: 6 #Units: 22 LR: 0.1 Train loss: 0.6946455836296082 Test accuracy: 0.518\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 1: train loss: 0.7598429918289185\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 2: train loss: 0.7473692297935486\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 3: train loss: 0.737312376499176\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 4: train loss: 0.7291935086250305\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 5: train loss: 0.7226300239562988\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 6: train loss: 0.717316746711731\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 7: train loss: 0.7130079865455627\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 8: train loss: 0.7095062136650085\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 9: train loss: 0.7066543102264404\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Epoch 10: train loss: 0.7043253183364868\n",
            "#Layers: 10 #Units: 2 LR: 0.1 Train loss: 0.7043253183364868 Test accuracy: 0.518\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 1: train loss: 0.6886953115463257\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 2: train loss: 0.6889837384223938\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 3: train loss: 0.6893118619918823\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 4: train loss: 0.6896519660949707\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 5: train loss: 0.6899874210357666\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 6: train loss: 0.6903067231178284\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 7: train loss: 0.6906036734580994\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 8: train loss: 0.6908757090568542\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 9: train loss: 0.6911213994026184\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Epoch 10: train loss: 0.69134122133255\n",
            "#Layers: 10 #Units: 12 LR: 0.1 Train loss: 0.69134122133255 Test accuracy: 0.482\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 1: train loss: 0.7049207091331482\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 2: train loss: 0.7029682993888855\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 3: train loss: 0.7013618350028992\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 4: train loss: 0.7000340819358826\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 5: train loss: 0.6989321708679199\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 6: train loss: 0.6980135440826416\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 7: train loss: 0.6972450017929077\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 8: train loss: 0.6965996623039246\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 9: train loss: 0.6960554718971252\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Epoch 10: train loss: 0.6955954432487488\n",
            "#Layers: 10 #Units: 22 LR: 0.1 Train loss: 0.6955954432487488 Test accuracy: 0.518\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 1: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 2: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 3: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 4: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 5: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 6: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 7: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 8: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 9: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Epoch 10: train loss: 54.82904815673828\n",
            "#Layers: 2 #Units: 2 LR: 1 Train loss: 54.82904815673828 Test accuracy: 0.518\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 1: train loss: 0.6912911534309387\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 2: train loss: 0.691714882850647\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 3: train loss: 0.6930046081542969\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 4: train loss: 0.6934566497802734\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 5: train loss: 0.6936044096946716\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 6: train loss: 0.6936514377593994\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 7: train loss: 0.6936664581298828\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 8: train loss: 0.6936713457107544\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 9: train loss: 0.6936727166175842\n",
            "#Layers: 2 #Units: 12 LR: 1 Epoch 10: train loss: 0.6936731338500977\n",
            "#Layers: 2 #Units: 12 LR: 1 Train loss: 0.6936731338500977 Test accuracy: 0.482\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 1: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 2: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 3: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 4: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 5: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 6: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 7: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 8: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 9: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Epoch 10: train loss: 45.14038848876953\n",
            "#Layers: 2 #Units: 22 LR: 1 Train loss: 45.14038848876953 Test accuracy: 0.482\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 1: train loss: 0.7168365716934204\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 2: train loss: 0.6985745429992676\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 3: train loss: 0.694984495639801\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 4: train loss: 0.6940919756889343\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 5: train loss: 0.6938462853431702\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 6: train loss: 0.6937755346298218\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 7: train loss: 0.6937550902366638\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 8: train loss: 0.6937487721443176\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 9: train loss: 0.6937462091445923\n",
            "#Layers: 6 #Units: 2 LR: 1 Epoch 10: train loss: 0.6937451362609863\n",
            "#Layers: 6 #Units: 2 LR: 1 Train loss: 0.6937451362609863 Test accuracy: 0.482\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 1: train loss: 0.6928465962409973\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 2: train loss: 0.6937543749809265\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 3: train loss: 0.6939599514007568\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 4: train loss: 0.6939111948013306\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 5: train loss: 0.6940428018569946\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 6: train loss: 0.6943350434303284\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 7: train loss: 0.6938009858131409\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 8: train loss: 0.6938309073448181\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 9: train loss: 0.6936377882957458\n",
            "#Layers: 6 #Units: 12 LR: 1 Epoch 10: train loss: 0.69366854429245\n",
            "#Layers: 6 #Units: 12 LR: 1 Train loss: 0.69366854429245 Test accuracy: 0.482\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 1: train loss: 0.6970411539077759\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 2: train loss: 0.6957881450653076\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 3: train loss: 0.6950249671936035\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 4: train loss: 0.69507896900177\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 5: train loss: 0.695091962814331\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 6: train loss: 0.6939691305160522\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 7: train loss: 0.6939037442207336\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 8: train loss: 0.6935520172119141\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 9: train loss: 0.6933345794677734\n",
            "#Layers: 6 #Units: 22 LR: 1 Epoch 10: train loss: 0.6934432983398438\n",
            "#Layers: 6 #Units: 22 LR: 1 Train loss: 0.6934432983398438 Test accuracy: 0.49\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 1: train loss: 0.6933155059814453\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 2: train loss: 0.6935586333274841\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 3: train loss: 0.6936373114585876\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 4: train loss: 0.6936619877815247\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 5: train loss: 0.6936697959899902\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 6: train loss: 0.6936722993850708\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 7: train loss: 0.6936731338500977\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 8: train loss: 0.6936732530593872\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 9: train loss: 0.693673312664032\n",
            "#Layers: 10 #Units: 2 LR: 1 Epoch 10: train loss: 0.693673312664032\n",
            "#Layers: 10 #Units: 2 LR: 1 Train loss: 0.693673312664032 Test accuracy: 0.482\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 1: train loss: 0.6984915733337402\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 2: train loss: 0.6948778033256531\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 3: train loss: 0.6941325068473816\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 4: train loss: 0.6939612627029419\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 5: train loss: 0.6939193606376648\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 6: train loss: 0.6939073204994202\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 7: train loss: 0.6939036250114441\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 8: train loss: 0.6938980221748352\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 9: train loss: 0.6938930749893188\n",
            "#Layers: 10 #Units: 12 LR: 1 Epoch 10: train loss: 0.6938893795013428\n",
            "#Layers: 10 #Units: 12 LR: 1 Train loss: 0.6938893795013428 Test accuracy: 0.482\n",
            "#Layers: 10 #Units: 22 LR: 1 Epoch 1: train loss: 0.6926774382591248\n",
            "#Layers: 10 #Units: 22 LR: 1 Epoch 2: train loss: 0.6937516331672668\n",
            "#Layers: 10 #Units: 22 LR: 1 Epoch 3: train loss: 0.6939715147018433\n",
            "#Layers: 10 #Units: 22 LR: 1 Epoch 4: train loss: 0.6940115094184875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQWyVLNB9Gl",
        "colab_type": "text"
      },
      "source": [
        "### Observações e Limitações\n",
        "* Usei apenas 10% do dataset pois o tempo de treinamento do dataseet inteiro para uma Rede Neural simples é inviável\n",
        "* Durante o treino não armazenei a queda do loss no formato de lista para plotar a diferença, pois o uso de memória da GPU ficou no limite\n",
        "* Após alguns treinamentos a GPU passou a dar um erro de memória que persistia mesmo restartando o kernel, a única solução possível foi voltar para CPU, deixando o treinamento mais lento ainda\n",
        "* Apesar de ter um resultado um pouco melhor, nem a Regressão Logística nem o MLP conseguem atingir uma bom resultado neste dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebB2EYlmCnH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}